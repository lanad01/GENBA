import os
import pandas as pd
import streamlit as st
import json
import tiktoken
import time
from loguru import logger

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.document_loaders import UnstructuredPowerPointLoader
from langchain.document_loaders import JSONLoader
from langchain.document_loaders import CSVLoader
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS

from dotenv import load_dotenv
import warnings

load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
warnings.filterwarnings('ignore')

## custom modules
def main():
    st.set_page_config(page_title="ë¶„ì„ì–´ì‹œìŠ¤í„´íŠ¸", page_icon="pine.png", layout='wide')
        
    st.markdown(
        """
        <style>
        [data-testid="stSidebar"] {
            min-width: 250px;
            max-width: 250px;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    if "RagComplete" not in st.session_state:
        st.session_state.RagComplete = None

    uploaded_files = st.file_uploader(
        "ğŸ“‚ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['pdf', 'docx', 'pptx', 'json', 'csv', 'xlsx', 'txt'],
        accept_multiple_files=True,
        
    )

    # ğŸ”¹ íŒŒì¼ì´ ì‚­ì œë˜ë©´ ìë™ìœ¼ë¡œ ë“±ë¡ ìƒíƒœ ë¦¬ì…‹
    if not uploaded_files:
        st.session_state.RagComplete = None  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒíƒœ ì´ˆê¸°í™”

    process = st.button("ğŸ“Œ ë“±ë¡í•˜ê¸°")

    if process:
        if not uploaded_files:
            st.warning("âš ï¸ íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
            return

        with st.spinner("â³ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                files_text = get_text(uploaded_files)
                text_chunks = get_text_chunks(files_text)
                vectordb = get_vectorstore(text_chunks)

                # ë²¡í„° DB ì €ì¥
                vectordb.save_local("./vectordb")
                
                # ë¬¸ì„œ ëª©ë¡ ê´€ë¦¬
                document_list = load_document_list()
                new_documents = [file.name for file in uploaded_files]
                document_list.extend(new_documents)
                save_document_list(list(set(document_list)))  # ì¤‘ë³µ ì œê±°

                st.session_state.RagComplete = True
                logger.info("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.warning("ë“±ë¡í•œ íŒŒì¼ì´ í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.session_state.RagComplete = False
                return

    # âœ… ë¬¸ì„œ ë“±ë¡ ì™„ë£Œ í›„ "ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸"ë¡œ ì´ë™í•˜ëŠ” ë²„íŠ¼ í‘œì‹œ
    if st.session_state.RagComplete:
        # ë“±ë¡ ì™„ë£Œ ì‹œ ì‚¬ìš©ìì—ê²Œ ì§ê´€ì ì¸ í”¼ë“œë°± ì œê³µ
        st.success("âœ… ë“±ë¡ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.toast("ğŸ‰ ë¬¸ë§¥ ë“±ë¡ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        # âœ… ë²„íŠ¼ ì¶”ê°€: í´ë¦­í•˜ë©´ "ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸.py"ë¡œ ì´ë™
        if st.button("ğŸ” ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ ì´ë™í•˜ê¸°"):
            st.switch_page("./pages/ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸.py")  # Streamlit í˜ì´ì§€ ì´ë™

    elif st.session_state.RagComplete is None:
        st.info("ğŸ“¥ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë“±ë¡ì„ ì§„í–‰í•˜ì„¸ìš”!")

def get_text(docs):
    doc_list = []
    for doc in docs:
        file_name = f'../documents/{doc.name}'
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())

        print(f"ğŸ“‚ Uploaded: {file_name}")

        if file_name.endswith('.pdf'):
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()

        elif file_name.endswith('.docx'):
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()

        elif file_name.endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()

        elif file_name.endswith('.json'):
            loader = JSONLoader(file_path=file_name, jq_schema='.[]', text_content=False)
            documents = loader.load()
            for doc in documents:
                content_dict = json.loads(doc.page_content)
                standard_term = content_dict.get("standard_term", "ì•Œ ìˆ˜ ì—†ìŒ")
                similar_terms = ", ".join(content_dict.get("similar_terms", []))
                doc.page_content = f"{standard_term} (ìœ ì‚¬ì–´: {similar_terms})" if similar_terms else f"{standard_term}"

        elif file_name.endswith('.csv'):
            loader = CSVLoader(file_name, encoding='utf-8')
            documents = loader.load()

        elif file_name.endswith('.xlsx'):
            df = pd.read_excel(file_name)
            documents = []
            for _, row in df.iterrows():
                content = f"í…Œì´ë¸”ëª…: {row['í…Œì´ë¸”ëª…']}, ì»¬ëŸ¼ëª…: {row['ì»¬ëŸ¼ëª…']}, ì»¬ëŸ¼í•œê¸€ëª…: {row['ì»¬ëŸ¼í•œê¸€ëª…']}, ì„¤ëª…: {row['ì»¬ëŸ¼ì„¤ëª…']}, ë°ì´í„° íƒ€ì…: {row['DATATYPE']}"
                documents.append(Document(page_content=content, metadata={"source": file_name}))

        elif file_name.endswith('.txt'):
            with open(file_name, "r", encoding="utf-8") as f:
                content = f.read()
            documents = [Document(page_content=content, metadata={"source": file_name})]
        
        doc_list.extend(documents)

    return doc_list

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100
    )
    return text_splitter.split_documents(text)

def get_vectorstore(text_chunks):
    # embeddings = HuggingFaceEmbeddings(
    #     model_name="jhgan/ko-sroberta-multitask",
    #     model_kwargs={'device': 'cpu'},
    #     encode_kwargs={'normalize_embeddings': True}
    # )      
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb

def load_document_list():
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë¡œë“œ"""
    try:
        with open("./document_list.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_document_list(document_list):
    """ë¬¸ì„œ ëª©ë¡ ì €ì¥"""
    with open("./document_list.json", "w", encoding="utf-8") as f:
        json.dump(list(set(document_list)), f, ensure_ascii=False, indent=2)

if __name__ == '__main__':
    main()
