{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ {ability} ì— ëŠ¥ìˆ™í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 20ì ì´ë‚´ë¡œ ì‘ë‹µí•˜ì„¸ìš”\",\n",
    "        ),\n",
    "        # ëŒ€í™” ê¸°ë¡ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©, history ê°€ MessageHistory ì˜ key ê°€ ë¨\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model  # í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ runnable ê°ì²´ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì§ˆë¬¸ì‹œê°: 2025-02-25 13:26:51\n",
      "[DEBUG] [handle_chat_response] ì´ì „ ëŒ€í™” ê¸°ë¡:\n",
      "ì‚¬ìš©ì: ê·¸ë˜í”„ ì»´íŒŒì¼\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ê·¸ë˜í”„ ì»´íŒŒì¼ì€ ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ì‹œê°í™” ë˜ëŠ” ë°ì´í„° ë¶„ì„ì˜ í•œ ë¶€ë¶„ìœ¼ë¡œ, ë°ì´í„°ë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ë˜í”„ ì»´íŒŒì¼ì„ í†µí•´ ë³µì¡í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìœ¼ë©°, ë°ì´í„° ê°„ì˜ ê´€ê³„ë‚˜ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ë˜í”„ ì»´íŒŒì¼ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìœ¼ë©°, ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ì™€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Pythonì—ì„œëŠ” Matplotlib, Seaborn, Plotlyì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Rì—ì„œëŠ” ggplot2ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ëª‡ ê°€ì§€ ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ë°ì´í„° ì¤€ë¹„**: ê·¸ë˜í”„ì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
      "2. **ê·¸ë˜í”„ ìœ í˜• ì„ íƒ**: ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë§ëŠ” ê·¸ë˜í”„ ìœ í˜•(ì˜ˆ: ë§‰ëŒ€ ê·¸ë˜í”„, ì„  ê·¸ë˜í”„, ì‚°ì ë„ ë“±)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
      "3. **ë ˆì´ë¸” ë° ì œëª© ì¶”ê°€**: ê·¸ë˜í”„ì˜ ì¶•, ì œëª©, ë²”ë¡€ ë“±ì„ ì¶”ê°€í•˜ì—¬ ê·¸ë˜í”„ì˜ ì˜ë¯¸ë¥¼ ëª…í™•íˆ í•©ë‹ˆë‹¤.\n",
      "4. **ìŠ¤íƒ€ì¼ë§**: ê·¸ë˜í”„ì˜ ìƒ‰ìƒ, í°íŠ¸, í¬ê¸° ë“±ì„ ì¡°ì •í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‹œê°í™”í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš©ì: ê·¸ë˜í”„ ìœ í˜• ì„ íƒì€ ì–´ë–»ê²Œ í• ê¹Œìš”?\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ê·¸ë˜í”„ ìœ í˜•ì„ ì„ íƒí•˜ëŠ” ê²ƒì€ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë‹¤ìŒì€ ê·¸ë˜í”„ ìœ í˜•ì„ ì„ íƒí•  ë•Œ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤:\n",
      "\n",
      "1. **ë°ì´í„°ì˜ ì¢…ë¥˜**:\n",
      "   - **ë²”ì£¼í˜• ë°ì´í„°**: ë§‰ëŒ€ ê·¸ë˜í”„, íŒŒì´ ì°¨íŠ¸\n",
      "   - **ì—°ì†í˜• ë°ì´í„°**: íˆìŠ¤í† ê·¸ë¨, ì„  ê·¸ë˜í”„\n",
      "   - **ì‹œê°„ ì‹œê³„ì—´ ë°ì´í„°**: ì„  ê·¸ë˜í”„, ì˜ì—­ ê·¸ë˜í”„\n",
      "\n",
      "2. **ë¹„êµ**:\n",
      "   - ì—¬ëŸ¬ ê·¸ë£¹ ê°„ì˜ ë¹„êµë¥¼ ì›í•  ê²½ìš° ë§‰ëŒ€ ê·¸ë˜í”„ê°€ ìœ ìš©í•©ë‹ˆë‹¤.\n",
      "   - ë¹„ìœ¨ì„ ë¹„êµí•  ë•ŒëŠ” íŒŒì´ ì°¨íŠ¸ê°€ ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ë¶„í¬**:\n",
      "   - ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ íˆìŠ¤í† ê·¸ë¨ì´ë‚˜ ìƒì ê·¸ë¦¼ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ê´€ê³„**:\n",
      "   - ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•  ë•ŒëŠ” ì‚°ì ë„ê°€ ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **íŠ¸ë Œë“œ**:\n",
      "   - ì‹œê°„ì— ë”°ë¥¸ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ê³  ì‹¶ë‹¤ë©´ ì„  ê·¸ë˜í”„ê°€ íš¨ê³¼ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "6. **êµ¬ì„±**:\n",
      "   - ì „ì²´ì— ëŒ€í•œ ë¶€ë¶„ì˜ í¬ê¸°ë¥¼ ë³´ì—¬ì£¼ê³  ì‹¶ë‹¤ë©´ íŒŒì´ ì°¨íŠ¸ë‚˜ ìŠ¤íƒí˜• ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë§ëŠ” ê·¸ë˜í”„ ìœ í˜•ì„ ì„ íƒí•˜ë©´ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ê° ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¹ì • ì±„ë„ì˜ ê³ ê° í™œë™ ì—¬ë¶€ë¥¼ ë¹„êµí•˜ê³  ì‹¶ë‹¤ë©´ ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš©ì: ê¸°ì¤€ë…„ì›”ì— ë”°ë¥¸  CMIPì˜ ì¶”ì„¸ë¥¼ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤.\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
      "ì‚¬ìš©ì: 2024ë…„ 5ì›”ì˜ cmipê°€ ì–´ë–»ë‹¤ê³  í•˜ì…¨ì£ ?\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: 2024ë…„ 5ì›”ì˜ ë³€ì•¡ì¢…ì‹ CMIP ê°’ì€ 17327.30ì…ë‹ˆë‹¤. ì´ ê°’ì€ ê¸°ì¤€ë…„ì›”ë³„ ë³€ì•¡ì¢…ì‹ CMIPì˜ í‰ê·  ì¶”ì„¸ë¥¼ ë¶„ì„í•œ ê²°ê³¼ì—ì„œ ë„ì¶œëœ ê°’ì…ë‹ˆë‹¤. 2024ë…„ 5ì›”ë¶€í„° 2024ë…„ 10ì›”ê¹Œì§€ì˜ ë°ì´í„°ì—ì„œ CMIP ê°’ì€ ë¹„êµì  ì•ˆì •ì ì¸ ì¶”ì„¸ë¥¼ ë³´ì´ë©°, í° ë³€ë™ ì—†ì´ 17315.90ì—ì„œ 17327.54 ì‚¬ì´ì—ì„œ ì›€ì§ì˜€ìŠµë‹ˆë‹¤.\n",
      "ì‚¬ìš©ì: ê·¸ë˜í”„ í•´ì„ ê°€ëŠ¥í• ê¹Œìš”?\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.\n",
      "ì‚¬ìš©ì: ê·¸ë˜í”„ í•´ì„ ê°€ëŠ¥í• ê¹Œìš”?\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸: âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import traceback\n",
    "import streamlit as st\n",
    "from typing import Dict, Any, Optional, Union\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# ì‚¬ìš©ì íŒ¨í‚¤ì§€\n",
    "from utils.vector_handler import save_chat_to_vector_db, search_similar_questions\n",
    "\n",
    "# âœ… MongoDB Atlas ì—°ê²° ì„¤ì •\n",
    "uri = \"mongodb+srv://swkwon:1q2w3e$r@cluster0.3rvbn.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "db = client[\"chat_history\"]\n",
    "collection = db[\"conversations\"]\n",
    "\n",
    "# âœ… ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (thread_idë³„ë¡œ ê´€ë¦¬)\n",
    "memory_store = {}\n",
    "\n",
    "def get_memory(thread_id: str) -> ConversationBufferMemory:\n",
    "    \"\"\"\n",
    "    íŠ¹ì • thread_idì— ëŒ€í•œ ConversationBufferMemoryë¥¼ ë°˜í™˜.\n",
    "    ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±.\n",
    "    \"\"\"\n",
    "    if thread_id not in memory_store:\n",
    "        memory_store[thread_id] = ConversationBufferMemory(memory_key=f\"history_{thread_id}\", return_messages=True)\n",
    "    \n",
    "        # âœ… MongoDBì—ì„œ ì´ì „ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        existing_messages = collection.find({\"internal_id\": thread_id}).sort(\"timestamp\", 1)  # ì‹œê°„ìˆœ ì •ë ¬\n",
    "        if existing_messages:\n",
    "            for document in existing_messages:\n",
    "                for msg in document.get(\"messages\", []):\n",
    "                    if msg[\"role\"] == \"user\":\n",
    "                        memory_store[thread_id].chat_memory.add_user_message(msg[\"content\"])\n",
    "                    elif msg[\"role\"] == \"assistant\":\n",
    "                        memory_store[thread_id].chat_memory.add_ai_message(msg[\"content\"])\n",
    "\n",
    "    return memory_store[thread_id]\n",
    "\n",
    "# âœ… ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬\n",
    "try:\n",
    "    thread_id = 'temp_KSW_20250225_1118'\n",
    "    print(f\"ğŸ” ì§ˆë¬¸ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # âœ… thread_idë³„ memory ê°€ì ¸ì˜¤ê¸°\n",
    "    memory = get_memory(thread_id)\n",
    "\n",
    "    # âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "    messages = memory.load_memory_variables({}).get(f\"history_{thread_id}\", \"\")\n",
    "\n",
    "    # ë©”ì‹œì§€ ê°ì²´ë¥¼ ì½ê¸° ì‰¬ìš´ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    previous_context = \"\"\n",
    "    if messages:\n",
    "        for msg in messages:\n",
    "            if msg.type == 'human':\n",
    "                previous_context += f\"ì‚¬ìš©ì: {msg.content}\\n\"\n",
    "            elif msg.type == 'ai':\n",
    "                previous_context += f\"ì–´ì‹œìŠ¤í„´íŠ¸: {msg.content}\\n\"\n",
    "\n",
    "    print(f\"[DEBUG] [handle_chat_response] ì´ì „ ëŒ€í™” ê¸°ë¡:\\n{previous_context}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] [handle_chat_response] ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… thread_idë³„ memory ê°€ì ¸ì˜¤ê¸°\n",
    "memory = get_memory(thread_id)\n",
    "\n",
    "# âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "messages = memory.load_memory_variables({}).get(f\"history_{thread_id}\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history_Location Inquiry': [AIMessage(content='ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = \"mongodb+srv://swkwon:1q2w3e$r@cluster0.3rvbn.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history_12345': 'Human: ì•ˆë…•í•˜ì„¸ìš”?\\nAI: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6884\\2605556453.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_store[thread_id] = ConversationBufferMemory(memory_key=f\"history_{thread_id}\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# ê° thread_idë³„ ë©”ëª¨ë¦¬ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬\n",
    "memory_store = {}\n",
    "\n",
    "def get_memory(thread_id):\n",
    "    if thread_id not in memory_store:\n",
    "        memory_store[thread_id] = ConversationBufferMemory(memory_key=f\"history_{thread_id}\")\n",
    "    return memory_store[thread_id]\n",
    "\n",
    "# íŠ¹ì • thread_idì— ëŒ€í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©\n",
    "thread_id = \"12345\"\n",
    "memory = get_memory(thread_id)\n",
    "\n",
    "# íˆìŠ¤í† ë¦¬ ì¶”ê°€\n",
    "memory.save_context({\"input\": \"ì•ˆë…•í•˜ì„¸ìš”?\"}, {\"output\": \"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"})\n",
    "\n",
    "# í•´ë‹¹ thread_idì˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}  # ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "\n",
    "# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_session_history(session_ids: str) -> BaseChatMessageHistory:\n",
    "    print(session_ids)\n",
    "    if session_ids not in store:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°\n",
    "        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜\n",
    "\n",
    "\n",
    "with_message_history = (\n",
    "    RunnableWithMessageHistory(  # RunnableWithMessageHistory ê°ì²´ ìƒì„±\n",
    "        runnable,  # ì‹¤í–‰í•  Runnable ê°ì²´\n",
    "        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "        input_messages_key=\"input\",  # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤\n",
    "        history_messages_key=\"history\",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_message': AIMessage(content='The cosine of an angle in a right-angled triangle is the ratio of the length of the side adjacent to the angle to the length of the hypotenuse of the triangle. Alternatively, the cosine of an angle in a unit circle is the x-coordinate of the point where the terminal side of the angle intersects the circle. The cosine function is a trigonometric function that is periodic with a period of 2Ï€ and has a range of -1 to 1.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 14, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-615edc0d-5f6f-4fbf-b3be-3a61090639c6-0', usage_metadata={'input_tokens': 14, 'output_tokens': 96, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# chain ìƒì„±\n",
    "chain = RunnableParallel({\"output_message\": ChatOpenAI()})\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ì†Œì— ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ChatMessageHistoryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# ì²´ì¸ì— ëŒ€í™” ê¸°ë¡ ê¸°ëŠ¥ì„ ì¶”ê°€í•œ RunnableWithMessageHistory ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"input\"ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.(ìƒëµì‹œ Message ê°ì²´ë¡œ ì…ë ¥)\n",
    "    # input_messages_key=\"input\",\n",
    "    # ì¶œë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"output_message\"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ìƒëµì‹œ Message ê°ì²´ë¡œ ì¶œë ¥)\n",
    "    output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "# ì£¼ì–´ì§„ ë©”ì‹œì§€ì™€ ì„¤ì •ìœ¼ë¡œ ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "with_message_history.invoke(\n",
    "    # í˜¹ì€ \"what is the definition of cosine?\" ë„ ê°€ëŠ¥\n",
    "    [HumanMessage(content=\"what is the definition of cosine?\")],\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector + RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import streamlit as st\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "# âœ… ì‚¬ìš©ìë³„ ë²¡í„°DB ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "VECTOR_DB_BASE_PATH = \"./vector_dbs\"\n",
    "\n",
    "def get_vector_db_path(session_id):\n",
    "    \"\"\"ì„¸ì…˜ ID ê¸°ë°˜ìœ¼ë¡œ ê°œë³„ ë²¡í„°DB ê²½ë¡œ ìƒì„±\"\"\"\n",
    "    return os.path.join(VECTOR_DB_BASE_PATH, f\"{session_id}_vectorstore\")\n",
    "\n",
    "def initialize_vector_store(session_id):\n",
    "    \"\"\"ì‚¬ìš©ì(ì“°ë ˆë“œ)ë³„ FAISS ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¶ˆëŸ¬ì˜¤ê¸°\"\"\"\n",
    "    vector_db_path = get_vector_db_path(session_id)\n",
    "    \n",
    "    if os.path.exists(vector_db_path):\n",
    "        # allow_dangerous_deserialization ë§¤ê°œë³€ìˆ˜ ì¶”ê°€\n",
    "        return FAISS.load_local(\n",
    "            vector_db_path, \n",
    "            OpenAIEmbeddings(),\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    else:\n",
    "        return FAISS.from_texts([\"\"], OpenAIEmbeddings())\n",
    "\n",
    "def save_vector_store(session_id, vectorstore):\n",
    "    \"\"\"ì‚¬ìš©ì(ì“°ë ˆë“œ)ë³„ FAISS ë²¡í„°ìŠ¤í† ì–´ ì €ì¥\"\"\"\n",
    "    vector_db_path = get_vector_db_path(session_id)\n",
    "    os.makedirs(os.path.dirname(vector_db_path), exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    vectorstore.save_local(vector_db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from typing import Any, Optional, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class ThreadBasedQuestionAnsweringRunnable(Runnable):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì(ì“°ë ˆë“œ)ë³„ ë²¡í„°DBì™€ ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•˜ëŠ” Runnable\n",
    "    \"\"\"\n",
    "    def __init__(self, message_history_runnable: RunnableWithMessageHistory, session_id: str):\n",
    "        self._session_id = session_id\n",
    "        self.message_history = message_history_runnable\n",
    "        self.vectorstore = initialize_vector_store(session_id)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True)\n",
    "    \n",
    "    \n",
    "    def invoke(self, input_data: Dict, config: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        input_data: {\"query\": \"ì‚¬ìš©ìì˜ ì§ˆë¬¸\"}\n",
    "        \"\"\"\n",
    "        user_query = input_data[\"query\"]\n",
    "        \n",
    "        # ë²¡í„°DBì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰\n",
    "        search_results = self.vectorstore.similarity_search(user_query, k=2)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in search_results])\n",
    "\n",
    "        # ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ë²¡í„°DBì— ì €ì¥\n",
    "        self.vectorstore.add_texts([user_query])\n",
    "        save_vector_store(self._session_id, self.vectorstore)  # _session_id ì‚¬ìš©\n",
    "\n",
    "        # ë©”ì‹œì§€ ê¸°ë¡ ì²˜ë¦¬\n",
    "        config = config or {}\n",
    "        config[\"configurable\"] = {\"session_id\": self._session_id}\n",
    "        \n",
    "        # ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€í•˜ê³  LLM ì‘ë‹µ ì–»ê¸°\n",
    "        llm_response = self.message_history.invoke(\n",
    "            [HumanMessage(content=f\"{retrieved_context}\\n\\n{user_query}\")],\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"query\": user_query,\n",
    "            \"retrieved_context\": retrieved_context,\n",
    "            \"llm_response\": llm_response,\n",
    "            \"session_id\": self._session_id\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”µ [ì‚¬ìš©ì 1] ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\n",
      "\n",
      "ğŸŸ¢ [ì‚¬ìš©ì 2] ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œ?',\n",
       " 'retrieved_context': 'ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œ?\\n\\nìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  ì‹¶ì–´.',\n",
       " 'llm_response': AIMessage(content='ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì€ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ì „ì— ìˆ˜í–‰í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¤ëŠ” ê¸°ëŠ¥ì€ ì œê³µë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\n\\n1. **ì´ìƒì¹˜ ì œê±°**: ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” IQR(Interquartile Range) ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n2. **ë¶„ì„ ê²°ê³¼ í™•ì¸**: ì´ìƒì¹˜ë¥¼ ì œê±°í•œ í›„, ë°ì´í„°ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„°ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ì„ í™•ì¸í•˜ê±°ë‚˜, ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në‹¤ìŒì€ ì´ëŸ¬í•œ ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤:\\n\\n```python\\nimport pandas as pd\\n\\n# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\\n# df_new = pd.read_csv(\\'your_new_data.csv\\')  # ì‹¤ì œ ë°ì´í„° ë¡œë“œ\\n\\ndef remove_outliers_iqr(df, column):\\n    # 1ì‚¬ë¶„ìœ„ìˆ˜ì™€ 3ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°\\n    Q1 = df[column].quantile(0.25)\\n    Q3 = df[column].quantile(0.75)\\n    IQR = Q3 - Q1\\n\\n    # ì´ìƒì¹˜ ê²½ê³„ ì„¤ì •\\n    lower_bound = Q1 - 1.5 * IQR\\n    upper_bound = Q3 + 1.5 * IQR\\n\\n    # ì´ìƒì¹˜ ì œê±°\\n    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\\n    return df_filtered\\n\\n# ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì´ìƒì¹˜ ì œê±°\\nfor col in df_new.select_dtypes(include=[\\'float64\\', \\'int64\\']).columns:\\n    df_new = remove_outliers_iqr(df_new, col)\\n\\n# ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„°í”„ë ˆì„ í™•ì¸\\nprint(\"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„°í”„ë ˆì„:\")\\nprint(df_new.describe())\\n\\n# ìƒê´€ê´€ê³„ ë¶„ì„\\ncorrelation_matrix = df_new.corr()\\nprint(\"ìƒê´€ê´€ê³„ í–‰ë ¬:\")\\nprint(correlation_matrix)\\n\\n# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™” (ì˜µì…˜)\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\', fmt=\".2f\")\\nplt.title(\\'Correlation Matrix\\')\\nplt.show()\\n```\\n\\nì´ ì½”ë“œëŠ” ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³ , ì´ìƒì¹˜ê°€ ì œê±°ëœ ë°ì´í„°ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ê³¼ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. `describe()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, `corr()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, `seaborn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 617, 'prompt_tokens': 1344, 'total_tokens': 1961, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1280}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a641c0a6-994a-4e3a-a8c1-334b0bfc35c9-0', usage_metadata={'input_tokens': 1344, 'output_tokens': 617, 'total_tokens': 1961, 'input_token_details': {'audio': 0, 'cache_read': 1280}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " 'session_id': 'abc123'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = {}  # ì„¸ì…˜ ê¸°ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ì†Œì— ì—†ìœ¼ë©´ ìƒˆë¡œìš´ ChatMessageHistoryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    # ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    model,  # ChatOpenAI ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    get_session_history,  # ëŒ€í™” ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "    # ì…ë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"input\"ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.(ìƒëµì‹œ Message ê°ì²´ë¡œ ì…ë ¥)\n",
    "    # input_messages_key=\"input\",\n",
    "    # ì¶œë ¥ ë©”ì‹œì§€ì˜ í‚¤ë¥¼ \"output_message\"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ìƒëµì‹œ Message ê°ì²´ë¡œ ì¶œë ¥)\n",
    "    # output_messages_key=\"output_message\",\n",
    ")\n",
    "\n",
    "# âœ… ì“°ë ˆë“œë³„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì‚¬ìš©ì 1, ì‚¬ìš©ì 2)\n",
    "user_1_runnable = ThreadBasedQuestionAnsweringRunnable(with_message_history, 'abc123')\n",
    "user_2_runnable = ThreadBasedQuestionAnsweringRunnable(with_message_history, 'abc123')\n",
    "\n",
    "# âœ… ì‚¬ìš©ì 1ì˜ ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\n",
    "print(\"\\nğŸ”µ [ì‚¬ìš©ì 1] ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\")\n",
    "user_1_runnable.invoke({\"query\": \"df_custì— ëŒ€í•œ ì»¬ëŸ¼ë³„ë¡œ ê²°ì¸¡ì¹˜ê°€ 20% ì´ìƒì¸ ì»¬ëŸ¼ì€ ì œê±°í•˜ëŠ” ì½”ë“œë¥¼ ìƒì„±í•´ì¤˜.\"})\n",
    "user_1_runnable.invoke({\"query\": \"ì´ì „ ë¶„ì„ ê²°ê³¼ì—ì„œ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì–´.\"})\n",
    "\n",
    "# âœ… ì‚¬ìš©ì 2ì˜ ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\n",
    "print(\"\\nğŸŸ¢ [ì‚¬ìš©ì 2] ì§ˆë¬¸ ì €ì¥ ë° ê²€ìƒ‰\")\n",
    "user_2_runnable.invoke({\"query\": \"ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ê³  ì‹¶ì–´.\"})\n",
    "user_2_runnable.invoke({\"query\": \"ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œ?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.vector_handler import initialize_vector_store\n",
    "\n",
    "thread_id = 'new_chat'\n",
    "query = 'columns_with_20_percent_missing ê°€ ë­ë¼êµ¬ í–ˆì£ ?'\n",
    "vectorstore = initialize_vector_store(thread_id)  # ì„¸ì…˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ\n",
    "\n",
    "\n",
    "# ğŸ” ê²€ìƒ‰ ì‹¤í–‰\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "retrieved_context = \"\\n\\n\".join([doc.page_content for doc in search_results])\n",
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x24744a56fc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VECTOR_DB_SESSION_PATH = './vector_db_session' \n",
    "vector_db_path = os.path.join(VECTOR_DB_SESSION_PATH, f\"new_chat_vectorstore\")\n",
    "\n",
    "DDS = FAISS.load_local(vector_db_path, OpenAIEmbeddings(), allow_dangerous_deserialization=True)  # âœ… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¡œì»¬ ë°ì´í„°ì´ë¯€ë¡œ í—ˆìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document ID: 94c4d233-b717-42f3-a7f4-a7b1e8916706\n",
      "Content: \n"
     ]
    }
   ],
   "source": [
    "all_docs = vectorstore.docstore._dict\n",
    "all_docs\n",
    "\n",
    "for doc_id, doc in all_docs.items():\n",
    "    print(f\"\\nDocument ID: {doc_id}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
