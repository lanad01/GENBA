{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=openai_api_key, temperature=0.0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 {ability} 에 능숙한 어시스턴트입니다. 20자 이내로 응답하세요\",\n",
    "        ),\n",
    "        # 대화 기록을 변수로 사용, history 가 MessageHistory 의 key 가 됨\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),  # 사용자 입력을 변수로 사용\n",
    "    ]\n",
    ")\n",
    "runnable = prompt | model  # 프롬프트와 모델을 연결하여 runnable 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_retrieved_documents(filtered_results, query):\n",
    "    \"\"\"검색된 문서를 LLM을 활용하여 요약\"\"\"\n",
    "    if not filtered_results:\n",
    "        return \"\"\n",
    "\n",
    "    document_texts = \"\\n\\n\".join([\n",
    "        f\"[유사도: {score:.2f}]\\n{doc.page_content}\" \n",
    "        for doc, score in filtered_results\n",
    "    ])\n",
    "\n",
    "    print(f'#################################################\\ndocument_texts: \\n{document_texts}')\n",
    "\n",
    "    # LLM을 사용하여 문서 요약\n",
    "    prompt = f\"\"\"\n",
    "    다음은 이전 대화 내역에서 현재 질문 \"{query}\"와 관련성이 높은 부분들입니다. 이를 참고하여 다음 지침에 따라 요약해주세요:\n",
    "\n",
    "    1. 현재 질문에 직접적으로 관련된 정보를 우선적으로 추출하세요.\n",
    "    2. 코드 블록과 그 설명은 온전히 보존하세요.\n",
    "    3. 유사도 점수가 높은 내용에 더 큰 가중치를 두세요.\n",
    "    4. 정보를 다음 형식으로 구조화하세요:\n",
    "    - 핵심 개념/용어 설명\n",
    "    - 관련 코드 예시\n",
    "    - 주요 인사이트/팁\n",
    "    5. 기술적 정확성을 유지하면서 중복 정보는 제거하세요.\n",
    "    6. 최신 대화 내용을 더 관련성 높게 처리하세요.\n",
    "\n",
    "    {document_texts}\n",
    "    \"\"\"\n",
    "    summarized_result = model.invoke(prompt)\n",
    "    return summarized_result.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vector_handler import initialize_vector_store\n",
    "\n",
    "def search_similar_questions(internal_id, query, top_k=5, similarity_threshold=0.7):\n",
    "    \"\"\"벡터DB에서 사용자의 질문과 유사한 질문 검색\"\"\"\n",
    "    vectorstore = initialize_vector_store(internal_id)  # 세션별 벡터스토어 로드\n",
    "    \n",
    "    # 🔎 유사도 점수와 함께 검색 실행\n",
    "    search_results = vectorstore.similarity_search_with_score(query, k=top_k*2)  # 더 많은 결과를 가져와서 필터링\n",
    "    \n",
    "    # 유사도 점수가 threshold를 넘는 결과만 필터링\n",
    "    filtered_results = []\n",
    "    seen_content = set()  # 중복 콘텐츠 확인용 집합\n",
    "    \n",
    "    for doc, score in search_results:\n",
    "        # FAISS의 score는 L2 거리이므로 코사인 유사도로 변환 (1 - score/2가 코사인 유사도의 근사값)\n",
    "        cosine_sim = 1 - (score / 2)\n",
    "        \n",
    "        if cosine_sim >= similarity_threshold:\n",
    "            # 콘텐츠 핵심 부분 추출 (사용자 질문 부분만)\n",
    "            content_key = \"\"\n",
    "            for line in doc.page_content.split('\\n'):\n",
    "                if \"사용자 질문:\" in line:\n",
    "                    content_key = line.strip()\n",
    "                    break\n",
    "            \n",
    "            # 중복 콘텐츠 건너뛰기\n",
    "            if content_key and content_key in seen_content:\n",
    "                continue\n",
    "            \n",
    "            # 가중치 계산 (콘텐츠 품질 기반)\n",
    "            weight = 1.0\n",
    "            if \"validated_code: None\" in doc.page_content or \"코드 없음\" in doc.page_content:\n",
    "                weight *= 0.8  # 코드가 없는 경우 가중치 감소\n",
    "            \n",
    "            if \"인사이트: None\" in doc.page_content or \"인사이트 없음\" in doc.page_content:\n",
    "                weight *= 0.9  # 인사이트가 없는 경우 가중치 감소\n",
    "                \n",
    "            if \"실행된 코드:\" in doc.page_content and \"코드 없음\" not in doc.page_content:\n",
    "                weight *= 1.3  # 실행된 코드가 있는 경우 가중치 증가\n",
    "                \n",
    "            if \"생성된 인사이트:\" in doc.page_content and \"인사이트 없음\" not in doc.page_content:\n",
    "                weight *= 1.2  # 인사이트가 있는 경우 가중치 증가\n",
    "            \n",
    "            # 최종 스코어 조정\n",
    "            adjusted_score = cosine_sim * weight\n",
    "            \n",
    "            if content_key:\n",
    "                seen_content.add(content_key)\n",
    "            \n",
    "            filtered_results.append((doc, adjusted_score))\n",
    "    \n",
    "    # 조정된 점수로 상위 결과 선택\n",
    "    filtered_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    filtered_results = filtered_results[:3]  # 상위 top_k개만 유지\n",
    "    \n",
    "    # 결과가 있는 경우에만 컨텍스트 생성\n",
    "    if filtered_results:\n",
    "        retrieved_context = \"\\n\\n\".join([\n",
    "            f\"[유사도: {score:.2f}]\\n{doc.page_content}\" \n",
    "            for doc, score in filtered_results\n",
    "        ])\n",
    "    else:\n",
    "        retrieved_context = \"\"\n",
    "    retrieved_context = summarize_retrieved_documents(filtered_results, query)\n",
    "    \n",
    "    return retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 [initialize_vector_store] 벡터DB 로드 시작 (세션: temp_KSW_20250225_1118)\n",
      "#################################################\n",
      "document_texts: \n",
      "[유사도: 1.19]\n",
      "\n",
      "            사용자 질문: 기준년월에 따른  CMIP의 추세를 알고 싶습니다.\n",
      "            AI 응답: 분석이 완료되었습니다! 아래 결과를 확인해주세요.\n",
      "            실행된 코드: ```python\n",
      "import pandas as pd\n",
      "\n",
      "# 기준년월별 변액종신CMIP의 평균 추세 계산\n",
      "cmip_trend = df.groupby('기준년월')['변액종신CMIP'].mean().round(2)\n",
      "\n",
      "# 결과 저장\n",
      "analytic_results = {\n",
      "    'CMIP_Trend': cmip_trend\n",
      "}\n",
      "\n",
      "# 집계성 데이터 출력\n",
      "print(cmip_trend)\n",
      "```\n",
      "            분석 결과: {'CMIP_Trend': 기준년월\n",
      "202405    17327.30\n",
      "202406    17320.91\n",
      "202407    17322.96\n",
      "202408    17323.08\n",
      "202409    17327.54\n",
      "202410    17315.90\n",
      "Name: 변액종신CMIP, dtype: float64}\n",
      "            생성된 인사이트: 1. 주요 발견사항\n",
      "   - CMIP(변액종신CMIP)의 추세를 분석한 결과, 2024년 5월부터 2024년 10월까지의 데이터에서 큰 변동 없이 비교적 안정적인 추세를 보이고 있습니다. CMIP 값은 17315.90에서 17327.54 사이에서 움직이고 있으며, 월별로 큰 변화는 관찰되지 않았습니다.\n",
      "\n",
      "2. 특이점\n",
      "   - 2024년 10월에 CMIP 값이 약간 감소한 17315.90을 기록하였으나, 이는 전체적인 추세에 큰 영향을 미치지 않는 수준입니다. 전반적으로 CMIP 값은 안정적인 수준을 유지하고 있습니다.\n",
      "\n",
      "3. 추천 사항\n",
      "   - CMIP의 안정적인 추세를 유지하기 위해 현재의 보험 상품 및 투자 전략을 지속적으로 모니터링하고, 외부 경제 환경 변화에 대한 민감도를 분석하여 필요 시 조정할 수 있는 준비가 필요합니다.\n",
      "   - 추가적으로, CMIP의 장기적인 추세를 파악하기 위해 더 긴 기간의 데이터를 분석하고, 계절적 요인이나 외부 경제 지표와의 상관관계를 검토하는 것도 유용할 것입니다.\n",
      "   - 만약 CMIP의 변동성이 증가할 경우, 그 원인을 파악하고 대응 전략을 마련하는 것이 중요합니다. 이를 위해 다양한 시나리오 분석을 통해 리스크 관리 방안을 강화할 것을 권장합니다.\n",
      "            리포트: 1. 요약\n",
      "2024년 5월부터 2024년 10월까지의 변액종신CMIP 데이터 분석 결과, CMIP 값은 비교적 안정적인 추세를 보이며 큰 변동 없이 17315.90에서 17327.54 사이에서 움직였습니다. 2024년 10월에 약간의 감소가 있었으나, 전체적인 추세에 큰 영향을 미치지 않았습니다. 이러한 안정적인 추세를 유지하기 위해 지속적인 모니터링과 외부 경제 환경 변화에 대한 민감도 분석이 필요합니다.\n",
      "\n",
      "2. 분석 방법\n",
      "분석은 2024년 5월부터 2024년 10월까지의 월별 CMIP 데이터를 기반으로 하여, 시간에 따른 추세를 파악하는 방식으로 진행되었습니다. 데이터의 변동성을 확인하기 위해 월별 CMIP 값을 비교하였으며, 이를 통해 안정성 여부를 평가하였습니다.\n",
      "\n",
      "3. 주요 발견사항\n",
      "- CMIP 값은 17315.90에서 17327.54 사이에서 움직이며, 월별로 큰 변화는 없었습니다.\n",
      "- 2024년 10월에 CMIP 값이 약간 감소한 17315.90을 기록했으나, 이는 전체적인 추세에 큰 영향을 미치지 않았습니다.\n",
      "- 전반적으로 CMIP는 안정적인 수준을 유지하고 있습니다.\n",
      "\n",
      "4. 결론 및 제언\n",
      "CMIP의 안정적인 추세를 유지하기 위해 현재의 보험 상품 및 투자 전략을 지속적으로 모니터링하고, 외부 경제 환경 변화에 대한 민감도를 분석하여 필요 시 조정할 수 있는 준비가 필요합니다. 또한, CMIP의 장기적인 추세를 파악하기 위해 더 긴 기간의 데이터를 분석하고, 계절적 요인이나 외부 경제 지표와의 상관관계를 검토하는 것도 유용할 것입니다. 만약 CMIP의 변동성이 증가할 경우, 그 원인을 파악하고 대응 전략을 마련하는 것이 중요합니다. 이를 위해 다양한 시나리오 분석을 통해 리스크 관리 방안을 강화할 것을 권장합니다.\n",
      "            \n",
      "\n",
      "[유사도: 1.19]\n",
      "\n",
      "            사용자 질문: 그래프 해석 가능할까요?\n",
      "            AI 응답: ❌ 활성화된 마트가 없습니다. 먼저 마트를 활성화해주세요.\n",
      "            실행된 코드: None\n",
      "            분석 결과: None\n",
      "            생성된 인사이트: None\n",
      "            리포트: None\n",
      "            \n",
      "\n",
      "[유사도: 1.16]\n",
      "\n",
      "            사용자 질문: 그래프 컴파일\n",
      "            AI 응답: 그래프 컴파일은 일반적으로 데이터 시각화 또는 데이터 분석의 한 부분으로, 데이터를 그래프로 변환하여 시각적으로 표현하는 과정을 의미합니다. 그래프 컴파일을 통해 복잡한 데이터 세트를 더 쉽게 이해할 수 있으며, 데이터 간의 관계나 패턴을 시각적으로 파악할 수 있습니다.\n",
      "\n",
      "그래프 컴파일을 수행하는 방법은 여러 가지가 있으며, 주로 사용하는 도구와 프로그래밍 언어에 따라 다릅니다. 예를 들어, Python에서는 Matplotlib, Seaborn, Plotly와 같은 라이브러리를 사용하여 그래프를 생성할 수 있습니다. R에서는 ggplot2가 널리 사용됩니다.\n",
      "\n",
      "그래프를 컴파일할 때 고려해야 할 몇 가지 요소는 다음과 같습니다:\n",
      "\n",
      "1. **데이터 준비**: 그래프에 사용할 데이터를 정리하고 필요한 형식으로 변환합니다.\n",
      "2. **그래프 유형 선택**: 데이터의 특성과 분석 목적에 맞는 그래프 유형(예: 막대 그래프, 선 그래프, 산점도 등)을 선택합니다.\n",
      "3. **레이블 및 제목 추가**: 그래프의 축, 제목, 범례 등을 추가하여 그래프의 의미를 명확히 합니다.\n",
      "4. **스타일링**: 그래프의 색상, 폰트, 크기 등을 조정하여 가독성을 높입니다.\n",
      "\n",
      "이러한 과정을 통해 데이터를 효과적으로 시각화하고, 이를 기반으로 인사이트를 도출할 수 있습니다.\n",
      "            실행된 코드: None\n",
      "            분석 결과: None\n",
      "            생성된 인사이트: None\n",
      "            리포트: None\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "internal_id = \"temp_KSW_20250225_1118\"\n",
    "query = \"그래프 해석 가능할까요?\"\n",
    "retrieved_context = search_similar_questions(internal_id, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 핵심 개념/용어 설명\n",
      "- **그래프 해석**: 그래프 해석은 데이터를 시각적으로 표현하여 데이터 간의 관계나 패턴을 파악하는 과정입니다. 이를 통해 복잡한 데이터 세트를 더 쉽게 이해할 수 있습니다.\n",
      "- **그래프 컴파일**: 그래프 컴파일은 데이터를 그래프로 변환하여 시각적으로 표현하는 과정입니다. Python에서는 Matplotlib, Seaborn, Plotly와 같은 라이브러리를 사용하여 그래프를 생성할 수 있습니다.\n",
      "\n",
      "### 관련 코드 예시\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# 기준년월별 변액종신CMIP의 평균 추세 계산\n",
      "cmip_trend = df.groupby('기준년월')['변액종신CMIP'].mean().round(2)\n",
      "\n",
      "# 결과 저장\n",
      "analytic_results = {\n",
      "    'CMIP_Trend': cmip_trend\n",
      "}\n",
      "\n",
      "# 집계성 데이터 출력\n",
      "print(cmip_trend)\n",
      "```\n",
      "\n",
      "### 주요 인사이트/팁\n",
      "1. **데이터 준비**: 그래프에 사용할 데이터를 정리하고 필요한 형식으로 변환합니다.\n",
      "2. **그래프 유형 선택**: 데이터의 특성과 분석 목적에 맞는 그래프 유형(예: 막대 그래프, 선 그래프, 산점도 등)을 선택합니다.\n",
      "3. **레이블 및 제목 추가**: 그래프의 축, 제목, 범례 등을 추가하여 그래프의 의미를 명확히 합니다.\n",
      "4. **스타일링**: 그래프의 색상, 폰트, 크기 등을 조정하여 가독성을 높입니다.\n",
      "\n",
      "### 추가 인사이트\n",
      "- CMIP(변액종신CMIP)의 추세를 분석한 결과, 2024년 5월부터 2024년 10월까지의 데이터에서 큰 변동 없이 비교적 안정적인 추세를 보이고 있습니다. CMIP 값은 17315.90에서 17327.54 사이에서 움직이고 있으며, 월별로 큰 변화는 관찰되지 않았습니다.\n",
      "- CMIP의 안정적인 추세를 유지하기 위해 지속적인 모니터링과 외부 경제 환경 변화에 대한 민감도 분석이 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 현재 접근 가능 마트 목록: ['cust_enroll_history', 'cust_intg', 'product_info']\n",
      "✅ 그래프 컴파일 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\권상우\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:1782: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        feature        VIF\n",
      "0          고객ID       1.01\n",
      "1         수익자여부       1.01\n",
      "2        CB신용평점       1.01\n",
      "3        CB신용등급       2.81\n",
      "4         두낫콜여부       1.01\n",
      "..          ...        ...\n",
      "113    변액종신보유여부       1.01\n",
      "114  변액종신최대납입회차       1.01\n",
      "115   변액종신유지계약수       1.01\n",
      "116  변액종신기납입보험료       1.01\n",
      "117        기준년월  271226.19\n",
      "\n",
      "[118 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 데이터프레임에서 수치형 변수만 선택\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# VIF 계산을 위해 결측값 처리 (임시로 평균값으로 대체)\n",
    "numeric_df = numeric_df.fillna(numeric_df.mean())\n",
    "\n",
    "# VIF 계산\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = numeric_df.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(numeric_df.values, i) for i in range(numeric_df.shape[1])]\n",
    "\n",
    "# '변액종신CMIP' 컬럼의 VIF 값 확인\n",
    "vif_value = vif_data[vif_data['feature'] == '변액종신CMIP']['VIF'].values[0]\n",
    "\n",
    "# 결과 저장\n",
    "analytic_results = {\n",
    "    \"VIF Analysis\": vif_data.round(2).head().to_dict(orient='list'),\n",
    "    \"VIF of 변액종신CMIP\": round(vif_value, 2)\n",
    "}\n",
    "\n",
    "print(vif_data.round(2))\n",
    "\n",
    "\"\"\"\n",
    "# 환경 설정\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from ai_agent_v2 import DataAnayticsAssistant\n",
    "\n",
    "# OpenAI API 키 로드\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "PROCESSED_DATA_PATH = \"../output/stage1/processed_data_info.xlsx\"\n",
    "mart_name = \"cust_intg\"\n",
    "def load_processed_data_info():\n",
    "    \"\"\"사전에 분석된 데이터 정보 로드\"\"\"\n",
    "    if not os.path.exists(PROCESSED_DATA_PATH):\n",
    "        return None\n",
    "    else:\n",
    "        # 모든 시트 로드\n",
    "        return pd.read_excel(PROCESSED_DATA_PATH, sheet_name=mart_name)\n",
    "\n",
    "# ✅ Streamlit 실행 시 데이터 로드\n",
    "mart_info = load_processed_data_info()\n",
    "\n",
    "# 어시스턴트 초기화\n",
    "assistant = DataAnayticsAssistant(openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import traceback\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Dict, Any, Optional, List\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# ✅ MongoDB 연결 설정\n",
    "uri = \"mongodb+srv://swkwon:1q2w3e$r@cluster0.3rvbn.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "db = client[\"chat_history\"]\n",
    "collection = db[\"conversations\"]\n",
    "\n",
    "def get_recent_history_weighted(thread_id: str) -> str:\n",
    "    \"\"\"\n",
    "    최근 5개의 대화 이력을 가져와, 최신 대화일수록 가중치를 높여서 Context로 구성.\n",
    "    \"\"\"\n",
    "    existing_messages = collection.find({\"internal_id\": thread_id}).sort(\"timestamp\", -1).limit(5)\n",
    "    \n",
    "    messages = []\n",
    "    for document in existing_messages:\n",
    "        messages.extend(document.get(\"messages\", []))\n",
    "    \n",
    "    # 메시지를 질문-답변 쌍으로 그룹화\n",
    "    message_pairs = []\n",
    "    for i in range(0, len(messages), 2):\n",
    "        if i + 1 < len(messages):  # 답변이 있는 경우만 쌍으로 추가\n",
    "            message_pairs.append((messages[i], messages[i+1]))\n",
    "    \n",
    "    # 최신 5개의 질문-답변 쌍만 유지\n",
    "    message_pairs = message_pairs[:5]\n",
    "    \n",
    "    # ✅ 가중치 설정 (최신 대화일수록 높게)\n",
    "    weights = [2.0, 1.5, 1.2, 1.0, 0.8]  # 최신 질문-답변일수록 가중치를 높게 설정\n",
    "    weights = weights[:len(message_pairs)]  # 쌍의 수에 맞게 조정\n",
    "    \n",
    "    weighted_context = \"\"\n",
    "    for i, (question, answer) in enumerate(reversed(message_pairs)):  # 과거 → 최신 순서로 정렬\n",
    "        # 질문 처리\n",
    "        content = f\"사용자 (가중치 {weights[i]}): {question['content']}\\n\"\n",
    "        \n",
    "        # 답변 처리\n",
    "        content += f\"어시스턴트 (가중치 {weights[i]}): {answer['content']}\\n\"\n",
    "\n",
    "        # 코드가 있는 경우 추가\n",
    "        if answer.get(\"validated_code\"):\n",
    "            content += f\"실행된 코드:\\n{answer['validated_code']}\\n\"\n",
    "        \n",
    "        # 분석 결과가 있는 경우 추가\n",
    "        if answer.get(\"analytic_result\"):\n",
    "            content += f\"분석 결과:\\n{answer['analytic_result']}\\n\"\n",
    "        \n",
    "        # 인사이트가 있는 경우 추가\n",
    "        if answer.get(\"insights\"):\n",
    "            content += f\"생성된 인사이트:\\n{answer['insights']}\\n\"\n",
    "        \n",
    "        weighted_context += content + \"\\n\"\n",
    "\n",
    "    return weighted_context\n",
    "\n",
    "\n",
    "# ✅ 새로운 질문에 대해 Context 기반 질문 재구성\n",
    "def handle_chat_response(assistant: Any, query: str, internal_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    기존 질문-답변을 반영하여 새로운 질문을 재구성하고 응답을 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"🤵 질문시각: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"🤵 Context Window 처리 시작\")\n",
    "\n",
    "        # ✅ 최근 대화 내역 가져오기\n",
    "        chat_history = get_recent_history_weighted(internal_id)\n",
    "        # ✅ LLM에게 현재 질문이 어떤 흐름에서 나왔는지 인식시키기 위한 프롬프트\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"당신은 사용자의 질문 흐름을 파악하여, 현재 질문이 기존 대화 맥락에서 어떤 의미를 가지는지 분석하는 AI 비서입니다. \n",
    "            \n",
    "            아래는 사용자의 최근 5개의 질문-답변 기록입니다. \n",
    "            이를 참고하여, 현재 질문이 어떤 의도로 이루어진 것인지 분석하고, \n",
    "            이전 코드 또는 분석 흐름을 유지하면서 어떻게 반영해야 하는지 결정하세요.\n",
    "            \n",
    "            - 기존 흐름을 유지하면서 현재 질문이 추가적으로 요구하는 것이 무엇인지 분석\n",
    "            - 기존 코드 또는 분석이 필요한 경우, 어떤 부분을 수정해야 하는지 판단\n",
    "            - 필요하면, 기존 결과 형식을 유지하면서 적절한 조정을 수행\n",
    "            \"\"\"),\n",
    "            (\"user\", \"### 최근 대화 기록\\n{previous_context}\"),\n",
    "            (\"user\", \"### 현재 질문\\n{query}\"),\n",
    "            (\"user\", \"### 분석해야 할 질문 의도 및 수정할 코드 영역\\n(기존 흐름을 유지하며 반영할 사항을 요약해 주세요.)\")\n",
    "        ])\n",
    "\n",
    "        # ✅ LLM을 활용하여 질문 의도를 분석\n",
    "        model = assistant.llm\n",
    "        analyzed_intent = model.invoke({\n",
    "            \"previous_context\": previous_context,\n",
    "            \"query\": query\n",
    "        }).content.strip()\n",
    "\n",
    "        # print(f\"🤖 분석된 질문 의도:\\n{analyzed_intent}\")\n",
    "\n",
    "        # # ✅ LLM을 활용하여 새로운 질문 재구성\n",
    "        # prompt = ChatPromptTemplate.from_messages([\n",
    "        #     (\"system\", \"\"\"당신은 질문 흐름을 반영하여 질문을 자연스럽게 재구성하는 AI 비서입니다. \n",
    "            \n",
    "        #     아래 분석된 질문 의도를 참고하여, 기존 흐름을 유지하면서 질문을 적절하게 보강하세요.\n",
    "            \n",
    "        #     - 질문 의도를 유지하면서 추가적인 설명을 보완\n",
    "        #     - 필요하면, 분석 코드에 적용해야 하는 사항을 명확히 포함\n",
    "        #     - 불필요한 확장은 지양하고, 사용자의 요청에 충실한 질문으로 보강\n",
    "        #     \"\"\"),\n",
    "        #     (\"user\", \"### 분석된 질문 의도\\n{analyzed_intent}\"),\n",
    "        #     (\"user\", \"### 보강된 질문\\n(기존 흐름을 유지하면서 자연스럽게 강화된 질문을 작성)\")\n",
    "        # ])\n",
    "\n",
    "        # final_query = model.invoke({\n",
    "        #     \"analyzed_intent\": analyzed_intent\n",
    "        # }).content.strip()\n",
    "\n",
    "        # print(f\"🤵 재구성된 질문:\\n{final_query}\")\n",
    "\n",
    "        # # ✅ 최종 질문을 기반으로 LLM 호출\n",
    "        # result = assistant.ask(final_query)\n",
    "        # print(f\"🤵 결과:\\n{result}\")\n",
    "\n",
    "        # # ✅ 응답 데이터 정리\n",
    "        # response_data = {\n",
    "        #     \"role\": \"assistant\",\n",
    "        #     \"content\": result.get(\"response\", \"응답을 생성할 수 없습니다.\"),\n",
    "        #     \"validated_code\": result.get(\"validated_code\"),\n",
    "        #     \"analytic_result\": result.get(\"analytic_result\"),\n",
    "        #     \"chart_filename\": result.get(\"chart_filename\"),\n",
    "        #     \"insights\": result.get(\"insights\"),\n",
    "        #     \"report\": result.get(\"report\"),\n",
    "        #     \"request_summary\": result.get(\"request_summary\"),\n",
    "        # }\n",
    "\n",
    "        # # ✅ MongoDB에 대화 이력 저장\n",
    "        # collection.update_one(\n",
    "        #     {\"internal_id\": internal_id},\n",
    "        #     {\n",
    "        #         \"$push\": {\n",
    "        #             \"messages\": {\n",
    "        #                 \"$each\": [\n",
    "        #                     {\n",
    "        #                         \"role\": \"user\", \n",
    "        #                         \"content\": query, \n",
    "        #                         \"timestamp\": datetime.now()\n",
    "        #                     },\n",
    "        #                     {\n",
    "        #                         \"role\": \"assistant\",\n",
    "        #                         \"content\": response_data[\"content\"],\n",
    "        #                         \"validated_code\": response_data[\"validated_code\"],\n",
    "        #                         \"chart_filename\": response_data[\"chart_filename\"],\n",
    "        #                         \"insights\": response_data[\"insights\"],\n",
    "        #                         \"report\": response_data[\"report\"],\n",
    "        #                         \"request_summary\": response_data[\"request_summary\"],\n",
    "        #                         \"timestamp\": datetime.now(),\n",
    "        #                     }   \n",
    "        #                 ]\n",
    "        #             }\n",
    "        #         }\n",
    "        #     },\n",
    "        #     upsert=True\n",
    "        # )\n",
    "\n",
    "        # return response_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {traceback.format_exc()}\")\n",
    "        return { \"role\": \"assistant\", \"content\": f\"❌ 오류 발생: {traceback.format_exc()}\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사용자 (가중치 2.0): 분석 결과를 보기가 어렵네요. 결과 데이터프레임을 transpose 해주세요~\\n어시스턴트 (가중치 2.0): 분석이 완료되었습니다! 아래 결과를 확인해주세요.\\n실행된 코드:\\n```python\\nimport pandas as pd\\n\\n# cust_enroll_history 데이터프레임의 전치\\ncust_enroll_history_transposed = cust_enroll_history.transpose()\\n\\n# cust_intg 데이터프레임의 전치\\ncust_intg_transposed = cust_intg.transpose()\\n\\n# 결과 저장\\nanalytic_results = {\\n    \\'cust_enroll_history_transposed\\': cust_enroll_history_transposed.head().round(2),\\n    \\'cust_intg_transposed\\': cust_intg_transposed.head().round(2)\\n}\\n\\n# 집계 데이터 출력\\nprint(cust_enroll_history_transposed)\\nprint(cust_intg_transposed)\\n```\\n생성된 인사이트:\\n1. 주요 발견사항\\n   - 데이터프레임의 전치를 통해 각 고객의 정보를 행 단위로 쉽게 비교할 수 있게 되었습니다. 이는 고객별로 가입한 보험 상품의 종류와 금액, 그리고 고객의 인구통계학적 특성을 한눈에 파악할 수 있도록 도와줍니다.\\n   - `cust_enroll_history_transposed` 데이터프레임은 고객의 보험 가입 내역을, `cust_intg_transposed` 데이터프레임은 고객의 통합 정보를 담고 있습니다. 각각의 데이터프레임은 고객 ID를 기준으로 정렬되어 있어, 고객별로 데이터를 쉽게 추적할 수 있습니다.\\n\\n2. 특이점\\n   - `cust_intg_transposed` 데이터프레임에서 일부 고객의 CB신용평점이 NaN으로 표시되어 있습니다. 이는 해당 고객의 신용평점 데이터가 누락되었음을 의미하며, 데이터 분석 시 주의가 필요합니다.\\n   - `cust_enroll_history_transposed` 데이터프레임의 경우, 고객이 가입한 보험의 종류와 금액이 다양하게 분포되어 있어, 특정 보험 상품에 대한 선호도나 트렌드를 분석할 수 있는 기회가 있습니다.\\n\\n3. 추천 사항\\n   - CB신용평점이 NaN인 고객에 대해서는 추가적인 데이터 수집이 필요할 수 있습니다. 신용평점은 고객의 리스크 평가에 중요한 요소이므로, 이를 보완하는 것이 중요합니다.\\n   - 고객별로 가입한 보험 상품의 종류와 금액을 분석하여, 특정 연령대나 성별에 따른 보험 상품 선호도를 파악할 수 있습니다. 이를 통해 맞춤형 보험 상품을 제안하거나 마케팅 전략을 수립하는 데 활용할 수 있습니다.\\n   - 데이터프레임의 전치 결과를 활용하여, 고객 세그먼트별로 보험 상품의 판매 전략을 최적화할 수 있는 방안을 모색해보는 것이 좋습니다. 예를 들어, 특정 연령대나 성별에서 인기 있는 보험 상품을 중심으로 프로모션을 진행할 수 있습니다.\\n\\n사용자 (가중치 1.5): 가입담보명이 \\'질병사망\\'인 것을 Y값으로 해서, 고객 정보(cust_intg) 피쳐와 결합하여, 어떤 정보들이 질병사망 담보 가입에 영향력을 끼쳤는지 분석해줘.\\n어시스턴트 (가중치 1.5): 분석이 완료되었습니다! 아래 결과를 확인해주세요.\\n실행된 코드:\\n```python\\nfrom sklearn.utils import resample\\nimport pandas as pd\\n\\n# \\'질병사망\\' 담보 가입 여부를 Y값으로 설정\\ncust_enroll_history[\\'질병사망_가입여부\\'] = cust_enroll_history[\\'가입담보명\\'].apply(lambda x: 1 if x == \\'질병사망\\' else 0)\\n\\n# 고객 정보와 결합\\nmerged_data = pd.merge(cust_intg, cust_enroll_history[[\\'고객ID\\', \\'질병사망_가입여부\\']], on=\\'고객ID\\', how=\\'left\\')\\n\\n# 클래스 불균형 해결: 다수 클래스 다운샘플링\\nmajority_class = merged_data[merged_data[\\'질병사망_가입여부\\'] == 0]\\nminority_class = merged_data[merged_data[\\'질병사망_가입여부\\'] == 1]\\n\\n# 다수 클래스 다운샘플링\\nif len(minority_class) > 0:\\n    majority_downsampled = resample(majority_class, \\n                                    replace=False, \\n                                    n_samples=len(minority_class), \\n                                    random_state=42)\\n\\n    # 다운샘플링된 데이터와 소수 클래스 데이터 결합\\n    balanced_data = pd.concat([majority_downsampled, minority_class])\\nelse:\\n    balanced_data = merged_data\\n\\n# 결과 저장\\nanalytic_results = {\\n    \\'balanced_data\\': balanced_data.head().round(2)\\n}\\n\\nprint(balanced_data.round(2))\\n```\\n생성된 인사이트:\\n1. 주요 발견사항\\n   - 현재 데이터셋에서 \\'질병사망\\' 담보 가입 여부가 하나의 클래스만 포함되어 있어, 이를 Y값으로 설정하여 분석하는 것이 불가능한 상황입니다. 이는 클래스 불균형 문제로, 모델 학습에 필요한 다양한 클래스가 존재하지 않기 때문에 발생합니다.\\n\\n2. 특이점\\n   - 데이터셋에 \\'질병사망\\' 담보 가입 여부가 모두 0으로 되어 있어, 실제로 이 담보에 가입한 고객이 데이터에 포함되지 않았거나, 데이터 수집 과정에서 누락되었을 가능성이 있습니다.\\n   - 고객 정보와 결합된 데이터셋에서 \\'질병사망\\' 담보 가입 여부가 모두 동일한 값으로 나타나, 데이터의 다양성이 부족합니다.\\n\\n3. 추천 사항\\n   - 추가 데이터를 수집하여 \\'질병사망\\' 담보에 가입한 고객의 데이터를 확보하는 것이 필요합니다. 이를 통해 클래스의 다양성을 확보할 수 있습니다.\\n   - 데이터 증강 기법을 활용하여 인위적으로 소수 클래스의 데이터를 생성하는 방법도 고려할 수 있습니다. 예를 들어, SMOTE(Synthetic Minority Over-sampling Technique)와 같은 기법을 사용하여 소수 클래스의 데이터를 생성할 수 있습니다.\\n   - 데이터 수집 과정에서 \\'질병사망\\' 담보 가입 여부가 제대로 기록되었는지 확인하고, 데이터 입력 오류가 있는지 검토하는 것이 필요합니다.\\n   - 만약 추가 데이터 수집이 어려운 경우, 다른 분석 목표를 설정하거나, 다른 Y값을 설정하여 분석을 진행하는 것도 고려할 수 있습니다.\\n\\n사용자 (가중치 1.2): 가입담보명이 \\'질병사망\\'인 것을 Y값으로 해서, 고객 정보(cust_intg) 피쳐와 결합하여, 어떤 정보들이 질병사망 담보 가입에 영향력을 끼쳤는지 분석해줘.\\n어시스턴트 (가중치 1.2): {\\'error_type\\': \\'ValueError\\', \\'error_message\\': \\'This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\\', \\'traceback\\': \\'Traceback (most recent call last):\\\\n  File \"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\RAG\\\\\\\\PINE_GENBA\\\\\\\\genba\\\\\\\\src\\\\\\\\utils\\\\\\\\analytic_agent.py\", line 375, in execute_sample_code\\\\n    self._execute_code_with_capture(code_to_execute, exec_globals, is_sample=True)\\\\n  File \"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\RAG\\\\\\\\PINE_GENBA\\\\\\\\genba\\\\\\\\src\\\\\\\\utils\\\\\\\\analytic_agent.py\", line 1005, in _execute_code_with_capture\\\\n    raise e\\\\n  File \"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\RAG\\\\\\\\PINE_GENBA\\\\\\\\genba\\\\\\\\src\\\\\\\\utils\\\\\\\\analytic_agent.py\", line 968, in _execute_code_with_capture\\\\n    exec(code, exec_globals, safe_locals)  # **제한된 네임스페이스에서 실행**\\\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"<string>\", line 35, in <module>\\\\n  File \"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\sklearn\\\\\\\\base.py\", line 1474, in wrapper\\\\n    return fit_method(estimator, *args, **kwargs)\\\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\\\n  File \"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\anaconda3\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\sklearn\\\\\\\\linear_model\\\\\\\\_logistic.py\", line 1246, in fit\\\\n    raise ValueError(\\\\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\\\\n\\'}\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = get_recent_history_weighted('temp_KSW_20250228_1715')\n",
    "chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "🤵 질문시각: 2025-02-28 19:12:53\n",
      "🤵 Context Window 처리 시작\n",
      "❌ 오류 발생: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_34780\\1695025663.py\", line 78, in handle_chat_response\n",
      "    role = \"사용자\" if msg[\"role\"] == \"user\" else \"어시스턴트\"\n",
      "                       ~~~^^^^^^^^\n",
      "TypeError: string indices must be integers, not 'str'\n",
      "\n",
      "{'role': 'assistant', 'content': '❌ 오류 발생: Traceback (most recent call last):\\n  File \"C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_34780\\\\1695025663.py\", line 78, in handle_chat_response\\n    role = \"사용자\" if msg[\"role\"] == \"user\" else \"어시스턴트\"\\n                       ~~~^^^^^^^^\\nTypeError: string indices must be integers, not \\'str\\'\\n'}\n"
     ]
    }
   ],
   "source": [
    "res = handle_chat_response(assistant = '' , query = '데이터프레임 결과가 맘에안들어' ,internal_id='temp_KSW_20250228_1715')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
