{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ë°ì´í„°í”„ë ˆì„ ê¸°ë³¸ ì •ë³´ ìƒì„± ì¤‘...\n",
      "ğŸ” RAG ê¸°ë°˜ ì»¬ëŸ¼ ì„¤ëª… ê²€ìƒ‰ ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542be7e957f043f582a5b66097068b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ” ì»¬ëŸ¼ ì„¤ëª… ê²€ìƒ‰ ì¤‘:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë“  ì •ë³´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\n",
      "âœ… [LOG] Sheet ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… [LOG] ëª¨ë“  ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ../output/stage1/eda_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "# LangChain ê´€ë ¨\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ\n",
    "from utils.vector_handler import load_vectorstore\n",
    "\n",
    "# ì—ëŸ¬ ì²˜ë¦¬\n",
    "import traceback\n",
    "\n",
    "# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì •\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ìƒìˆ˜ ì •ì˜\n",
    "PAGE_NAME = \"analysis\"\n",
    "VECTOR_DB_ANSS_PATH = f\"./vectordb/{PAGE_NAME}\"\n",
    "\n",
    "vectorstore = load_vectorstore(db_path=VECTOR_DB_ANSS_PATH)\n",
    "\n",
    "def summarize_data(df, df_name):\n",
    "    # ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„(df)ì— ëŒ€í•œ EDA(íƒìƒ‰ì  ë°ì´í„° ë¶„ì„) ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.\n",
    "    # \"ì»¬ëŸ¼ ê°œìš”\"ì— ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ì •ë³´ í¬í•¨\n",
    "    # ì—°ì†í˜• ë³€ìˆ˜ì˜ ì¸ìŠ¤í„´ìŠ¤(ì˜ˆì œ) ì œì™¸ (í† í° ìˆ˜ ì ˆê° ëª©ì )\n",
    "\n",
    "    # âœ… 1. ì´ìƒì¹˜ íƒì§€ (IQR ë°©ì‹)\n",
    "    outliers_info = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outlier_count = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "\n",
    "        if outlier_count > 0:\n",
    "            outliers_info[col] = int(outlier_count)\n",
    "\n",
    "    # âœ… 2. ì»¬ëŸ¼ ê°œìš” (ê¸°ë³¸ í†µê³„ ë° ê²°ì¸¡ ì •ë³´ + ì¸ìŠ¤í„´ìŠ¤ ì˜ˆì œ + ë²”ì£¼í˜• ë¶„í¬ ì¶”ê°€)\n",
    "    columns_info = []\n",
    "    for col in df.columns:\n",
    "        col_dtype = df[col].dtype\n",
    "\n",
    "        col_info = {\n",
    "            \"ë°ì´í„°í”„ë ˆì„ëª…\": df_name,\n",
    "            \"ì»¬ëŸ¼ëª…\": col,\n",
    "            \"ë°ì´í„° íƒ€ì…\": col_dtype,\n",
    "        }\n",
    "\n",
    "        # âœ… ë²”ì£¼í˜• ë³€ìˆ˜ë§Œ ì¸ìŠ¤í„´ìŠ¤(ì˜ˆì œ) í¬í•¨ (ì—°ì†í˜• ë³€ìˆ˜ ì œì™¸í•˜ì—¬ í† í° ìˆ˜ ì ˆê°)\n",
    "        if col_dtype in [\"object\", \"category\"]:\n",
    "            unique_vals = df[col].dropna().unique()\n",
    "            # ìœ ë‹ˆí¬ ê°’ ìƒ˜í”Œë§ (20ê°œ ì´ìƒì´ë©´ 10ê°œë§Œ ì¶œë ¥ + '...')\n",
    "            if len(unique_vals) > 20:\n",
    "                sample_display = list(unique_vals[:10]) + ['...']\n",
    "            else:\n",
    "                sample_display = unique_vals.tolist()\n",
    "            col_info[\"ì¸ìŠ¤í„´ìŠ¤(ì˜ˆì œ)\"] = sample_display\n",
    "\n",
    "            # âœ… ë²”ì£¼í˜• ë³€ìˆ˜ì¼ ê²½ìš°, ë¶„í¬ ì •ë³´ ì¶”ê°€ (ìµœëŒ€ 10ê°œë§Œ ì €ì¥í•˜ì—¬ í† í° ì ˆê°)\n",
    "            value_counts = df[col].value_counts(normalize=True) * 100\n",
    "            value_counts = value_counts[:10]  # ìµœëŒ€ 10ê°œë§Œ ìœ ì§€\n",
    "            category_distribution = {val: round(pct, 2) for val, pct in value_counts.items()}\n",
    "            col_info[\"ë²”ì£¼í˜• ë¶„í¬\"] = category_distribution\n",
    "\n",
    "        columns_info.append(col_info)\n",
    "\n",
    "    columns_info_df = pd.DataFrame(columns_info)\n",
    "\n",
    "    return columns_info_df\n",
    "\n",
    "def search_column_descriptions(stage1_results_dict, vectorstore):\n",
    "    \"\"\"RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¬ëŸ¼ ì„¤ëª…ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # ì „ì²´ ì²˜ë¦¬í•´ì•¼ í•  í–‰ ìˆ˜ ê³„ì‚°\n",
    "    total_rows = sum(len(df) for df in stage1_results_dict.values())\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"ğŸ” ì»¬ëŸ¼ ì„¤ëª… ê²€ìƒ‰ ì¤‘\") as pbar:\n",
    "        for df_name, df in stage1_results_dict.items():\n",
    "            df_after_llm = df[['ë°ì´í„°í”„ë ˆì„ëª…', 'ì»¬ëŸ¼ëª…']]\n",
    "            \n",
    "            for i, row in df_after_llm.iterrows():\n",
    "                table_name = row['ë°ì´í„°í”„ë ˆì„ëª…']\n",
    "                col = row['ì»¬ëŸ¼ëª…']\n",
    "                \n",
    "                search_query = f\"í…Œì´ë¸”ëª… : {table_name} | ì»¬ëŸ¼ëª… : {col}\"\n",
    "                docs = vectorstore.similarity_search(search_query, k=1)\n",
    "                \n",
    "                if docs:\n",
    "                    best_match = docs[0].page_content\n",
    "                    data.append({\n",
    "                        'ë°ì´í„°í”„ë ˆì„ëª…': table_name,\n",
    "                        'ì»¬ëŸ¼ëª…': col,\n",
    "                        'ì»¬ëŸ¼ì„¤ëª…': best_match.split('\\n')[3].split('ì„¤ëª…')[1].strip()\n",
    "                    })\n",
    "                else:\n",
    "                    data.append({\n",
    "                        'ë°ì´í„°í”„ë ˆì„ëª…': table_name,\n",
    "                        'ì»¬ëŸ¼ëª…': col,\n",
    "                        'ì»¬ëŸ¼ì„¤ëª…': \"ì„¤ëª… ì—†ìŒ\"\n",
    "                    })\n",
    "                pbar.update(1)  # ì§„í–‰ë°” ì—…ë°ì´íŠ¸\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def generate_dataframe_info(df, vectorstore=None):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°í”„ë ˆì„ì˜ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” í†µí•© í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„\n",
    "        vectorstore: RAG ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„°ìŠ¤í† ì–´ (ì„ íƒì‚¬í•­)\n",
    "    \n",
    "    Returns:\n",
    "        dict: ìƒì„±ëœ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š ë°ì´í„°í”„ë ˆì„ ê¸°ë³¸ ì •ë³´ ìƒì„± ì¤‘...\")\n",
    "    df_name = str(df.name) if hasattr(df, 'name') else 'Sheet'\n",
    "    basic_info = summarize_data(df, df_name)\n",
    "    \n",
    "    # RAG ê¸°ë°˜ ì»¬ëŸ¼ ì„¤ëª… ê²€ìƒ‰ (vectorstoreê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ)\n",
    "    if vectorstore is not None:\n",
    "        print(\"ğŸ” RAG ê¸°ë°˜ ì»¬ëŸ¼ ì„¤ëª… ê²€ìƒ‰ ì¤‘...\")\n",
    "        column_descriptions = search_column_descriptions({df_name: basic_info}, vectorstore)\n",
    "    else:\n",
    "        column_descriptions = None\n",
    "    \n",
    "    # ê²°ê³¼ í†µí•©\n",
    "    result = {\n",
    "        \"basic_info\": basic_info,\n",
    "        \"column_descriptions\": column_descriptions,\n",
    "        \"df_name\": df_name\n",
    "    }\n",
    "    \n",
    "    print(\"âœ… ëª¨ë“  ì •ë³´ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    return result\n",
    "\n",
    "def save_results_to_excel(results_dict, output_path=\"../output/stage1/eda_summary.xlsx\"):\n",
    "    \"\"\"\n",
    "    ìƒì„±ëœ ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì‹œíŠ¸ë¡œ í†µí•©í•˜ì—¬ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        results_dict (dict): generate_dataframe_info í•¨ìˆ˜ì˜ ê²°ê³¼\n",
    "        output_path (str): ì €ì¥í•  ì—‘ì…€ íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
    "    try:\n",
    "        # ê¸°ë³¸ ì •ë³´ì™€ ì»¬ëŸ¼ ì„¤ëª…ì„ ì»¬ëŸ¼ëª… ê¸°ì¤€ìœ¼ë¡œ í†µí•©\n",
    "        merged_df = results_dict[\"basic_info\"].copy()\n",
    "        \n",
    "        # ì»¬ëŸ¼ ì„¤ëª…ì´ ìˆëŠ” ê²½ìš° í†µí•©\n",
    "        if results_dict[\"column_descriptions\"] is not None:\n",
    "            merged_df = merged_df.merge(\n",
    "                results_dict[\"column_descriptions\"][['ë°ì´í„°í”„ë ˆì„ëª…', 'ì»¬ëŸ¼ëª…', 'ì»¬ëŸ¼ì„¤ëª…']],\n",
    "                on=['ë°ì´í„°í”„ë ˆì„ëª…', 'ì»¬ëŸ¼ëª…'],\n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            merged_df.to_excel(writer, sheet_name=results_dict[\"df_name\"], index=False)\n",
    "            print(f\"âœ… [LOG] {results_dict['df_name']} ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        print(f\"âœ… [LOG] ëª¨ë“  ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [LOG] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {output_path}\")\n",
    "        print(f\"âŒ [LOG] ì˜¤ë¥˜ ë‚´ìš©: {e}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "df = pd.read_pickle('../data/datamart_20250226_093329.pkl')\n",
    "result = generate_dataframe_info(df, vectorstore)  # vectorstoreëŠ” ì„ íƒì‚¬í•­\n",
    "save_results_to_excel(result, output_path=\"../output/stage1/eda_summary.xlsx\")  # ê²°ê³¼ ì €ì¥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
