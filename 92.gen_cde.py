import streamlit as st
import sys
import io
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv
import os
import pandas as pd
from ai_agent_v2 import DataAnayticsAssistant

# OpenAI API í‚¤ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv('OPENAI_API_KEY')

mart_name = "cust_intg"
df = pd.read_pickle(f'../data/{mart_name}.pkl')

# ë°ì´í„°í”„ë ˆì„ ì„¤ì •
llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0)

# ìƒì„±í˜• AIê°€ ìƒì„±í•œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ì¶œë ¥ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def execute_code_with_capture(code):
    captured_output = io.StringIO()
    original_stdout = sys.stdout  # ì›ë˜ í‘œì¤€ ì¶œë ¥ ì €ì¥
    sys.stdout = captured_output  # í‘œì¤€ ì¶œë ¥ ë³€ê²½

    analysis_results = {}  # ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ë³€ìˆ˜

    try:
        exec(code, globals())  # ìƒì„±ëœ ì½”ë“œ ì‹¤í–‰
    except Exception as e:
        print(f"Error: {str(e)}")  # ì—ëŸ¬ ë°œìƒ ì‹œ ì¶œë ¥

    sys.stdout = original_stdout  # í‘œì¤€ ì¶œë ¥ì„ ì›ë˜ ìƒíƒœë¡œ ë³µì›
    return captured_output.getvalue(), analysis_results  # ì‹¤í–‰ëœ print ê²°ê³¼ ë° analysis_results ë°˜í™˜

# AI í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
PROMPT_GENERATE_CODE = """
ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ íŒŒì´ì¬ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
í™œìš©í•  ë°ì´í„°í”„ë ˆì„ì€ 'df' ë³€ìˆ˜ë¡œ ì œê³µë©ë‹ˆë‹¤.

ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
1. ì˜ˆì œ ë°ì´í„°í”„ë ˆì„ ìƒì„±ì„ í•˜ì§€ë§ê³ , ì œê³µëœ ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
2. ë¶„ì„ ê²°ê³¼ë¥¼ dictionary í˜•íƒœì˜ 'analysis_results' ë³€ìˆ˜ì— ì €ì¥í•´ì£¼ì„¸ìš”. ì €ì¥ ì‹œ ì•„ë˜ì˜ **ê·œì¹™**ì„ ë”°ë¼ì£¼ì„¸ìš”.
   **ê·œì¹™**
   - ì§‘ê³„ì„± ë°ì´í„°ê°€ ì•„ë‹Œ ê²½ìš°ì—ëŠ” ë°˜ë“œì‹œ head()ë¥¼ í•œ ë’¤ì— ì €ì¥í•´ì£¼ì„¸ìš”. 
   - 'analysis_results'ëŠ” ê° ë¶„ì„ ë‹¨ê³„ë¥¼ key, í•´ë‹¹ ê²°ê³¼ë¥¼ valueë¡œ ê°–ëŠ” êµ¬ì¡°ì—¬ì•¼ í•©ë‹ˆë‹¤.
3. ì½”ë“œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.
4. ì§‘ê³„ì„± ë°ì´í„°ëŠ” ë°˜ë“œì‹œ printë¥¼ ì°ì–´ì£¼ì„¸ìš”.
"""

# Streamlit UI
st.title("AI ì½”ë“œ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„")

# ì‚¬ìš©ì ìš”ì²­ ì…ë ¥
user_request = st.text_area("ìˆ˜í–‰í•  ë¶„ì„ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”", "ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³ , ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë³€ìˆ˜ ìŒì„ íƒìƒ‰í•´ ì£¼ì„¸ìš”.")

if st.button("ì½”ë“œ ìƒì„± ë° ì‹¤í–‰"):
    # AIë¥¼ í†µí•´ ì½”ë“œ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", PROMPT_GENERATE_CODE),
        ("user", f"user_request:\n{user_request}\n\n")
    ])
    chain = prompt | llm
    response = chain.invoke({"user_request": user_request})

    # ìƒì„±ëœ ì½”ë“œ í‘œì‹œ
    ai_generated_code = response.content.split('```python')[1].split('```')[0]
    st.subheader("ğŸ“ ìƒì„±ëœ ì½”ë“œ")
    st.code(ai_generated_code, language="python")

    # ì‹¤í–‰í•  ì½”ë“œì˜ ì„¤ëª… ìš”ì²­
    explain_prompt = f"ë‹¤ìŒ íŒŒì´ì¬ ì½”ë“œì˜ ëª©ì ê³¼ ì£¼ìš” ë¡œì§ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n\n{ai_generated_code}"
    explanation = chain.invoke({"user_request": explain_prompt}).content

    # ì‹¤í–‰í•  ì½”ë“œ ì„¤ëª… ì¶œë ¥
    st.subheader("ğŸ“Œ ì½”ë“œ ì„¤ëª…")
    st.write(explanation)

    # ì‹¤í–‰í•œ ì½”ë“œì˜ ì¶œë ¥ ê²°ê³¼ ì €ì¥ ë° í‘œì‹œ
    st.subheader("ğŸ” ì½”ë“œ ì‹¤í–‰ ê²°ê³¼")
    output, analysis_results = execute_code_with_capture(ai_generated_code)

    # ì‹¤í–‰ ê³¼ì •ì—ì„œ printë¡œ ì¶œë ¥ëœ ë°ì´í„°
    if output.strip():
        st.text_area("ğŸ“¢ ì‹¤í–‰ ê³¼ì •ì—ì„œ printëœ ì¶œë ¥", output, height=200)
    else:
        st.write("ğŸ“Œ ì‹¤í–‰ ì¤‘ printëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # analysis_results ê°ì²´ ì¶œë ¥
    st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼ ë°ì´í„°")
    if analysis_results:
        for key, value in analysis_results.items():
            st.write(f"**{key}**:")
            st.write(value)
    else:
        st.write("ğŸ” ë¶„ì„ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
