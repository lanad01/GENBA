##########################################################################################
# Program Description
##########################################################################################
# 1. í”„ë¡œê·¸ë¨ ì„¤ëª…
##########################################################################################

##########################################################################################
# ë¼ì´ë¸ŒëŸ¬ë¦¬
##########################################################################################
import os, sys
import io
import pickle
import pandas as pd
import numpy as np
import traceback
from datetime import datetime
from typing import TypedDict, List, Literal, Annotated, Dict, Union
from pydantic import BaseModel
import tiktoken
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END, START
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.types import Command
from fuzzywuzzy import process

from prompt.prompts import *
from common_txt import logo
from utils.vector_handler import load_vectorstore

##########################################################################################
# ìƒìˆ˜ ë° ë³€ìˆ˜ ì„ ì–¸ë¶€
##########################################################################################
# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (Windows í™˜ê²½)
matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False

VECTOR_DB_BASE_PATH = "./vectordb/analysis"
PROCESSED_DATA_PATH = "../output/stage1/processed_data_info.xlsx"
MAX_RETRIES = 3
TOKEN_LIMIT = 10000 # âœ… í† í° ì œí•œ ì„¤ì • (ì˜ˆ: 5000 í† í° ì´ˆê³¼ ì‹œ ì°¨ë‹¨)
RECURSION_LIMIT = 100
eda_prompt_mapping = {
    "ê¸°ë³¸ ì •ë³´ ë¶„ì„": PROMPT_EDA_BASIC_INFO,
    "ê¸°ì´ˆ í†µê³„ ë¶„ì„": PROMPT_EDA_STATISTICAL_ANALYSIS,
    "ê²°ì¸¡ì¹˜ ì²˜ë¦¬": PROMPT_EDA_MISSING_VALUE_HANDLING,
    "ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„": PROMPT_EDA_FEATURE_RELATIONSHIP,
    "ì´ìƒì¹˜ íƒì§€": PROMPT_EDA_OUTLIER_DETECTION
}

##########################################################################################
# êµ¬í˜„ ì½”ë“œ
##########################################################################################
# âœ… AI ë¶„ì„ ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜(stateì— ì ì¬ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë™)
class State(TypedDict):
    messages: List[HumanMessage]  # ğŸ”¹ ì‚¬ìš©ìì™€ AI ê°„ì˜ ëŒ€í™” ë©”ì‹œì§€ ëª©ë¡    
    context_history: List[Dict]  # ì´ì „ ëŒ€í™” ê¸°ë¡
    filtered_context: str  # í•„í„°ë§ëœ ì»¨í…ìŠ¤íŠ¸
    final_query: str  # ìµœì¢… ì²˜ë¦¬ëœ ì§ˆì˜
    mart_info: str  # ğŸ”¹ í˜„ì¬ í™œì„±í™”ëœ ë°ì´í„°í”„ë ˆì„ (ë¶„ì„ ëŒ€ìƒ)
    generated_code: str  # ğŸ”¹ ì´ˆê¸° ìƒì„±ëœ ì½”ë“œ
    q_category: str  # ğŸ”¹ Supervisorê°€ íŒë‹¨í•œ ì§ˆë¬¸ ìœ í˜• (Analytics, General, Knowledge)
    general_response: str  # ğŸ”¹ General ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ
    knowledge_response: str  # ğŸ”¹ Knowledge ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ
    retry_count: int  # ğŸ”¹ ì½”ë“œ ì¬ìƒì„± ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜ (ìµœëŒ€ 3íšŒ)
    regenerated_code: str  # ğŸ”¹ ì¬ìƒì„±ëœ ì½”ë“œ
    validated_code: str  # ì „ì²´ ì‹¤í–‰ê¹Œì§€ í†µê³¼í•œ ì½”ë“œ
    analytic_result: Dict  # ğŸ”¹ ì „ì²´ ë°ì´í„°ë¥¼ ì‹¤í–‰í•˜ì—¬ ì–»ì€ ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    execution_output: str  # ğŸ”¹ ì½”ë“œ ì‹¤í–‰ ì¤‘ ìƒì„±ëœ ì¶œë ¥ í…ìŠ¤íŠ¸
    error_message: str  # ğŸ”¹ ì½”ë“œ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ë©”ì‹œì§€ (ìˆë‹¤ë©´ ì¬ì‹œë„í•  ë•Œ í™œìš©)
    data_id: str  # ğŸ”¹ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë•Œ ë¶€ì—¬ë˜ëŠ” ê³ ìœ  ID (íŒŒì¼ ì €ì¥ ì‹œ í™œìš©)
    insights: str  # ğŸ”¹ LLMì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸
    report: str  # ğŸ”¹ ìƒì„±ëœ ë¦¬í¬íŠ¸
    chart_needed: bool  # ğŸ”¹ ì°¨íŠ¸ê°€ í•„ìš”í•œì§€ ì—¬ë¶€ (True: í•„ìš”í•¨, False: ë¶ˆí•„ìš”)
    chart_filename: str  # ğŸ”¹ ìƒì„±ëœ ì°¨íŠ¸ì˜ íŒŒì¼ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
    chart_error: int  # ğŸ”¹ ì°¨íŠ¸ ìƒì„± íšŸìˆ˜ ì¹´ìš´í„°
    from_full_execution: bool  # ğŸ”¹ ì½”ë“œ ì¬ìƒì„± ì‹œ ì´ˆê¸° ì‹¤í–‰ ì—¬ë¶€
    from_token_limit: bool  # ğŸ”¹ í† í° ì œí•œ ì´ˆê³¼ ì‹œ ì´ˆê¸° ì‹¤í–‰ ì—¬ë¶€
    request_summary: str  # ğŸ”¹ ë¶„ì„ ìš”ì²­ì„ í•œê¸€ë¡œ ìš”ì•½í•œ ë‚´ìš©
    analysis_type: str  # ğŸ”¹ ë¶„ì„ ìœ í˜• (EDA, ML, General)
    eda_question: str  # ğŸ”¹ EDA ì½”ë“œ ìƒì„± ê²°ê³¼
    eda_stage: int  # ğŸ”¹ EDA ë‹¨ê³„ ì¹´ìš´í„°

# âœ… ê²½ë¡œ ê²°ì •ìš© ë¼ìš°í„°
class Router(BaseModel):
    next: Literal["Analytics", "General", "Knowledge", "Generate_Code", "Execute_Sample", "Regenerate_Code", "Execute_Full", 
                  "Save_Data", "Insight_Builder", "Chart_Builder", "Regenerate_Chart", "Report_Builder", "__end__"]

class DataAnayticsAssistant:
    """Python DataFrame ê¸°ë°˜ AI ë¶„ì„ ì—ì´ì „íŠ¸ (LangGraph ê¸°ë°˜)"""
    ###############################################################################################
    # âœ… ì´ˆê¸°í™”
    ###############################################################################################
    def __init__(self, openai_api_key: str):
        print("="*100)
        print("ğŸ”¹ ë¶„ì„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”")
        print("="*100)
        self.llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0)
        self.active_marts = None
        self.mart_info = None
        self.retry_count = 0

        # ì§ˆì˜ ë¶„ë¥˜(ë¬¸ë§¥ + ì‚¬ìš©ìì§ˆì˜ or ì‚¬ìš©ì ì§ˆì˜)
        self.original_query = None
        self.context = None
        
        # ë§ˆíŠ¸ ì •ë³´ ì´ˆê¸° ë¡œë“œ
        try:
            self.mart_info_df = pd.read_excel(PROCESSED_DATA_PATH, sheet_name=None)
            print(f"ğŸ”¹ í˜„ì¬ ì ‘ê·¼ ê°€ëŠ¥ ë§ˆíŠ¸ ëª©ë¡: {list(self.mart_info_df.keys())}")
        except Exception as e:
            print(f"âš ï¸ ë§ˆíŠ¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.mart_info_df = {}
            
        self.build_graph()

    ###############################################################################################
    # âœ… ê·¸ë˜í”„ êµ¬ì„±
    ###############################################################################################
    def build_graph(self):
        """LangGraphë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„ íë¦„ êµ¬ì„±"""
        workflow = StateGraph(State)

        # ë…¸ë“œ ì„ ì–¸
        workflow.add_node("Context", self.handle_context)
        workflow.add_node("Supervisor", self.supervisor)
        workflow.add_node("Analytics", self.handle_analytics)
        workflow.add_node("General", self.handle_general)
        workflow.add_node("Knowledge", self.handle_knowledge)
        workflow.add_node("Check_Analysis_Question", self.classify_analysis_question)  # âœ… ì¶”ê°€
        workflow.add_node("Eda_Generate_Code", self.generate_eda_code)  # âœ… ì¶”ê°€
        workflow.add_node("ML_Generate_Code", self.generate_ml_code)  # âœ… ì¶”ê°€
        workflow.add_node("Generate_Code", self.generate_python_code)
        workflow.add_node("Execute_Sample", self.execute_sample_code)
        workflow.add_node("Regenerate_Code", self.regenerate_code)
        workflow.add_node("Execute_Full", self.execute_full_data)
        workflow.add_node("Save_Data", self.save_data)
        workflow.add_node("Insight_Builder", self.generate_insights)
        workflow.add_node("Chart_Builder", self.generate_chart)
        workflow.add_node("Regenerate_Chart", self.regenerate_chart)
        workflow.add_node("Report_Builder", self.generate_report)

        # ê¸°ë³¸ íë¦„ ì •ì˜
        workflow.add_edge(START, "Context")
        workflow.add_edge("Context", "Supervisor")
        workflow.add_conditional_edges(
            "Supervisor",
            lambda state: state["q_category"],  # Supervisorê°€ ê²°ì •í•œ ê²½ë¡œë¡œ ì´ë™
            {
                "Analytics": "Analytics",
                "General": "General",
                "Knowledge": "Knowledge",
            }
        )
        
         # âœ… Analytics â†’ Check_EDA_Question ì¶”ê°€
        workflow.add_edge("Analytics", "Check_Analysis_Question")

        # âœ… Check_Analysis_Question â†’ ë¶„ì„ ì½”ë“œ ìƒì„± ë…¸ë“œ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Check_Analysis_Question",
            lambda state: (
                "Eda_Generate_Code" if state.get("analysis_type") == "EDA" else
                "ML_Generate_Code" if state.get("analysis_type") == "ML" else
                "Generate_Code"
            ),
            {
                "Eda_Generate_Code": "Eda_Generate_Code",
                "ML_Generate_Code": "ML_Generate_Code",
                "Generate_Code": "Generate_Code",
            }
        )

        # âœ… ì½”ë“œ ìƒì„± ë…¸ë“œ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        # workflow.add_edge("Eda_Generate_Code", "Execute_Sample")
        workflow.add_conditional_edges(
            "Eda_Generate_Code",
            self.route_after_generate_code,
            {
                "Execute_Sample": "Execute_Sample",
                END : END,
            }
        )
        # workflow.add_edge("ML_Generate_Code", "Execute_Sample")
        workflow.add_conditional_edges(
            "ML_Generate_Code",
            self.route_after_generate_code,
            {
                "Execute_Sample": "Execute_Sample",
                END : END,
            }
        )
        workflow.add_conditional_edges(
            "Generate_Code",
            self.route_after_generate_code,
            {
                "Execute_Sample": "Execute_Sample",
                END : END,
            }
        )

        # âœ… ìƒ˜í”Œ ì‹¤í–‰ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Execute_Sample",
            self.route_after_sample,
            {
                "Execute_Full": "Execute_Full",
                "Regenerate_Code": "Regenerate_Code",
                END : END
            }
        )

        # âœ… ì½”ë“œ ì¬ìƒì„± íë¦„
        workflow.add_conditional_edges(
            "Regenerate_Code",
            self.route_after_regenerate,  # ìƒˆë¡œìš´ ë¼ìš°í„° í•¨ìˆ˜ ì‚¬ìš©
            {
                "Execute_Sample": "Execute_Sample",
                "Execute_Full": "Execute_Full",
                END: END  # âœ… 3íšŒ ì´ìƒì´ë©´ ì¢…ë£Œ
            }
        )

        # âœ… ì „ì²´ ë°ì´í„° ì‹¤í–‰ í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Execute_Full",
            self.route_after_full_execution,
            {
                "Save_Data": "Save_Data",
                "Regenerate_Code": "Regenerate_Code",
                END : END
            }
        )

        # âœ… ë°ì´í„° ì €ì¥ í›„ ì¸ì‚¬ì´íŠ¸ ìƒì„± ë…¸ë“œë¡œ ì´ë™
        workflow.add_edge("Save_Data", "Insight_Builder")

        # âœ… ì¸ì‚¬ì´íŠ¸ ìƒì„± í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Insight_Builder",
            self.route_after_insights,
            {
                "Chart_Builder": "Chart_Builder",
                "Report_Builder": "Report_Builder"
            }
        )

        # âœ… ì°¨íŠ¸ ìƒì„± í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Chart_Builder",
            self.route_after_chart,
            {
                "Regenerate_Chart": "Regenerate_Chart",  # ì‹¤íŒ¨ ì‹œ ì¬ìƒì„±
                "Report_Builder": "Report_Builder",  # ì„±ê³µ ë˜ëŠ” ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼
            }
        )
        
        # âœ… ì°¨íŠ¸ ì¬ìƒì„± í›„ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì„¤ì •
        workflow.add_conditional_edges(
            "Regenerate_Chart",
            self.route_after_chart,
            {
                "Regenerate_Chart": "Regenerate_Chart",  # ì—¬ì „íˆ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì¬ìƒì„±
                "Report_Builder": "Report_Builder",  # ì„±ê³µ ë˜ëŠ” ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼
            }
        )

        # âœ… ë¦¬í¬íŠ¸ ìƒì„± í›„ ì¢…ë£Œ
        workflow.add_conditional_edges(
            "Report_Builder",
            self.route_after_report,  # âœ… ë³„ë„ í•¨ìˆ˜ì—ì„œ stateë¥¼ ë°›ì•„ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½
            {
                "Eda_Generate_Code": "Eda_Generate_Code",
                END : END,
            }
        )
        self.graph = workflow.compile()
        print("âœ… ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ")        
        
    ###############################################################################################
    # âœ… ì‹¤í–‰
    ###############################################################################################
    def ask(self, query: str, context: list):
        """LangGraph ì‹¤í–‰"""
        # ì¿¼ë¦¬ ìƒíƒœ ì €ì¥
        self.original_query = query
        self.context = context
        
        return self.graph.invoke({
            "messages": [HumanMessage(content=query)],  # ì›ë³¸ ì¿¼ë¦¬ë§Œ ì „ë‹¬
        }, config={"recursion_limit": RECURSION_LIMIT})

    ###############################################################################################
    # âœ… ë…¸ë“œ êµ¬í˜„
    ###############################################################################################
    #########################################################
    # âœ… Context Windows ë…¸ë“œ
    # -> Context_Filter
    #########################################################
    def handle_context(self, state: State) -> Command:
        """ê´€ë ¨ ìˆëŠ” ëŒ€í™”ë§Œ í•„í„°ë§í•˜ëŠ” ë…¸ë“œ"""
        print("ğŸ” ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§ ë‹¨ê³„")
        
        user_request = self.original_query
        context = self.context
        
        if not context:
            print("ğŸ” ì´ì „ ëŒ€í™” ê¸°ë¡ ì—†ìŒ")
            self.context_query = user_request  # contextê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
            return Command(
                update={"filtered_context": None},
                goto="Supervisor"
            )
            
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
ë‹¹ì‹ ì€ AI ë¹„ì„œì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìµœê·¼ ëŒ€í™” ê¸°ë¡ ì¤‘, í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ëŒ€í™”ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°í•˜ì„¸ìš”.
- ë‹¨, ì—°ê´€ëœ ì§ˆë¬¸ì¼ ê²½ìš° ë°˜ë“œì‹œ ì½”ë“œ(`validated_code`) ë° ë¶„ì„ ê²°ê³¼(`analytic_results`)ë¥¼ í•¨ê»˜ ìœ ì§€í•©ë‹ˆë‹¤.
            """),
            ("user", "### í˜„ì¬ ì§ˆë¬¸\n{user_request}"),
            ("user", "### ìµœê·¼ ëŒ€í™” ê¸°ë¡\n{context}"),
            ("user", "### í•„í„°ë§ëœ ë¬¸ë§¥ (ê´€ë ¨ ìˆëŠ” ë¬¸ë§¥ë§Œ ìœ ì§€)"),
        ])

        chain = prompt | self.llm
        filtered_context = chain.invoke({
            "user_request": user_request,
            "context": "\n".join([f"\nì‚¬ìš©ì: {chat['query']}\nì–´ì‹œìŠ¤í„´íŠ¸: {chat['response']}" for chat in self.context])
        }).content.strip()

        print(f"ğŸ” í•„í„°ë§ í›„ ëŒ€í™” :\n{filtered_context}")
        
        # context_query ì„¤ì • (í•„í„°ë§ëœ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° í¬í•¨)
        if filtered_context:
                self.context_query = f"""
# ğŸ“ ì´ì „ ëŒ€í™” ë‚´ì—­
{filtered_context}

# ğŸ¤” í˜„ì¬ ì§ˆë¬¸
{self.original_query}
        """
        else:
            self.context_query = self.original_query
        
        return Command(
            update={"filtered_context": filtered_context},
            goto="Supervisor"
        )

    #########################################################
    # âœ… Supervisor ë…¸ë“œ
    # -> General / Knowledge / Analytics 
    #########################################################
    def supervisor(self, state: State) -> Command:
        """ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” Supervisor"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ‘¨â€ğŸ’¼ Supervisor ë‹¨ê³„:")
        user_request = self.context_query or self.original_query # ë¬¸ë§¥ + ì§ˆì˜

        # Request Summary ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_REQUEST_SUMMARY),
            ("user", "{user_request}")
        ])
        
        chain = prompt | self.llm
        request_summary = chain.invoke({
            "user_request": user_request
        }).content.strip()
        
        print(f"ğŸ‘¨â€ğŸ’¼ ìš”ì•½ëœ ì§ˆì˜ ë‚´ìš©: {request_summary}")
        
        # ì§ˆë¬¸ ìœ í˜• ê²°ì •
        prompt = ChatPromptTemplate.from_messages([
                ("system", PROMPT_SUPERVISOR),
                ("user", " user_request:\n{user_request}\n\n")
        ])
        chain = prompt | self.llm.with_structured_output(Router)
        response = chain.invoke({"user_request": user_request})
        print(f"ğŸ‘¨â€ğŸ’¼ ë‹¤ìŒ ë‹¨ê³„(Analytics or General or Knowledge): {response.next}")
        return Command(
            update={
                "q_category": response.next, 
                "request_summary": request_summary,
                "eda_stage": 0
            }, 
            goto=response.next
        )
    
    #########################################################
    # âœ… General ë…¸ë“œ
    #########################################################
    def handle_general(self, state: State) -> Command:
        """ì¼ë°˜ì ì¸ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ"""
        print("\nğŸ’¬ [handle_general] ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬")
        prompt = ChatPromptTemplate.from_messages([
                ("system", PROMPT_GENERAL),
                ("user", " user_request:\n{user_request}\n\n")
        ])
                
        chain = prompt | self.llm
        # user_request = state['messages'][0].content
        user_request = self.original_query
        response = chain.invoke({"user_request": user_request})
        print(f"ğŸ’¬ ì¼ë°˜ ì‘ë‹µ: {response.content}")
        return Command(update={"general_response": response.content}, goto=END)

    #########################################################
    # âœ… Knowledge ë…¸ë“œ
    #########################################################
    def handle_knowledge(self, state: State) -> Command:
        """ì§€ì‹ ê¸°ë°˜ ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ"""
        print("\nğŸ“š [handle_knowledge] ì§€ì‹ ê¸°ë°˜ ì§ˆë¬¸ ì²˜ë¦¬")
        
        user_request = self.context_query or self.original_query

        # ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ(ë¯¸ë¦¬ ë¬¸ë§¥ ë“±ë¡ì´ í•„ìš”)
        vectorstore = load_vectorstore('./vectordb/analysis')
        if vectorstore is None:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì‹¤íŒ¨: FAISS ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLMìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤.")
            # ì¼ë°˜ LLM ì‘ë‹µ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                    ("system", PROMPT_GENERAL),
                    ("user", "{user_question}")
            ])
            chain = prompt | self.llm
            response = chain.invoke({"user_question": user_request})
            return Command(update={"knowledge_response": response.content}, goto=END)

        # Retriever ìƒì„±
        retriever = vectorstore.as_retriever()

        # ì‚¬ìš©ì ì§ˆë¬¸ ê²€ìƒ‰
        retrieved_docs = retriever.get_relevant_documents(user_request)

        if not retrieved_docs:
            response = "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            # ê²€ìƒ‰ëœ ë¬¸ì„œ ìƒìœ„ 3ê°œë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
            context = "\n".join([doc.page_content for doc in retrieved_docs[:3]])
            prompt = ChatPromptTemplate.from_messages([
                    ("system", PROMPT_KNOWLEDGE),
                    ("user", "\nì§ˆë¬¸:\n{user_question}"),
                    ("user", "\ndocument:\n{context}")
            ])
            chain = prompt | self.llm
            response = chain.invoke({"user_question": user_request, "context": context})
        print(f"ğŸ“– ì§€ì‹ ê¸°ë°˜ ì‘ë‹µ: {response.content}")
        return Command(update={"knowledge_response": response.content}, goto=END)
    
    #########################################################
    # âœ… Analytics ë…¸ë“œ
    # -> Check_Analysis_Question
    #########################################################
    def handle_analytics(self, state: State) -> Command:
        """ë¶„ì„ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œ"""
        print("ğŸ‘¨â€ğŸ’¼ [handle_analytics] ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì‹œì‘")
        return Command(goto="Check_Analysis_Question")

    #########################################################
    # âœ… classify_analysis_question ë…¸ë“œ
    # -> Eda_Generate_Code / ML_Generate_Code / Generate_Code
    #########################################################
    def classify_analysis_question(self, state: State) -> Command:
        """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ EDA, ML, ì¼ë°˜ ì§ˆë¬¸ì¸ì§€ íŒë‹¨ ë° ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ"""
        print("=" * 100)
        print("ğŸ‘¨â€ğŸ’¼ ë¶„ì„ ìœ í˜• íŒë‹¨ ë‹¨ê³„ (EDA vs ML vs ì¼ë°˜)")

        user_question = self.context_query or self.original_query

        # âœ… LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_CHECK_ANALYSIS_QUESTION),
            ("user", "user_question:\n{user_question}\n\n")
        ])

        chain = prompt | self.llm
        analysis_decision = chain.invoke({"user_question": user_question}).content.strip().upper()

        # âœ… ë¶„ì„ ìœ í˜•ì„ stateì— ì €ì¥
        state_update = {"analysis_type": analysis_decision if analysis_decision in ["EDA", "ML"] else "General"}

        # âœ… ë¶„ì„ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ì´ë™
        next_node = (
            "Eda_Generate_Code" if state_update["analysis_type"] == "EDA" else
            "ML_Generate_Code" if state_update["analysis_type"] == "ML" else
            "Generate_Code"
        )
        print(f"ğŸ‘¨â€ğŸ’¼ ë¶„ì„ ìœ í˜•: {state_update['analysis_type']}, ë‹¤ìŒ ë‹¨ê³„: {next_node}")

        return Command(update=state_update, goto=next_node)

    #########################################################
    # âœ… Eda_Generate_Code ë…¸ë“œ
    # -> Execute_Sample / END
    #########################################################
    def generate_eda_code(self, state: State) -> Command:
        """ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ Python ì½”ë“œ ìƒì„±"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ¤– ì½”ë“œ ìƒì„± ë‹¨ê³„:")

        # í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.active_marts:
            print("âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.")
            return Command(
                update={"error_message": "âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”."}, 
                goto='__end__'
            )

        user_request = self.context_query or self.original_query
        
        # ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰í•  EDA ë‹¨ê³„ ì„ íƒ
        selected_categories = self.map_to_eda_category(user_request)
        selected_categories = list(dict.fromkeys(selected_categories))
        # print(f"ğŸ¤– ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {selected_categories}")

        # ì„ íƒëœ í”„ë¡¬í”„íŠ¸ë§Œ ì‹¤í–‰
        if "ì „ì²´" in selected_categories:
            selected_prompts = [PROMPT_EDA_FEATURE_IMPORTANCE_ANALYSIS]
        elif "ê¸°íƒ€" in selected_categories:
            selected_prompts = [PROMPT_GENERATE_CODE]
        else:
            selected_prompts = [eda_prompt_mapping[category] for category in selected_categories if category in eda_prompt_mapping]
        # print(f"ğŸ¤– ì„ íƒëœ í”„ë¡¬í”„íŠ¸: {selected_prompts}")

        current_stage = state.get("eda_stage", 0)
        print(f"í˜„ì¬íšŒì°¨: {current_stage}, ìˆ˜í–‰íšŒì°¨: {len(selected_prompts)}")
        
        if current_stage >= len(selected_prompts):
            print("ğŸ¤– ëª¨ë“  EDA ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
            return Command(goto=END)
        
        # í˜„ì¬ ì‹¤í–‰í•  EDA ë‹¨ê³„ ì¶œë ¥
        # print(f"ğŸ¤– ì‹¤í–‰ ì¤‘: {selected_prompts[current_stage]}")
        prompt_text = selected_prompts[current_stage]

        # ë§ˆíŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì ìš©)
        mart_info = self._get_mart_info()

        prompt = ChatPromptTemplate.from_messages([
                    ("system", prompt_text),
                    ("user", "\nuser_request:\n{user_request}"),
                    ("user", "\mart_info:\n{mart_info}")
            ])
        chain = prompt | self.llm
        response = chain.invoke({
            "user_request": user_request,
            "mart_info": mart_info
        })
        print(f"ğŸ¤– ìƒì„±ëœ ì½”ë“œ:\n{response.content}\n")

        return Command(update={
            "generated_code": response.content,
            "eda_stage": current_stage + 1,
            "regenerated_code": None,  # ì´ˆê¸°í™”
            "validated_code": None     # ì´ˆê¸°í™”
        }, goto="Execute_Sample")

    #########################################################
    # âœ… ML_Generate_Code ë…¸ë“œ
    # -> Execute_Sample / END
    #########################################################
    def generate_ml_code(self, state):
        """ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ Python ì½”ë“œ ìƒì„±"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ¤– ì½”ë“œ ìƒì„± ë‹¨ê³„:")

        # í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.active_marts:
            print("âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.")
            return Command(
                update={"error_message": "âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”."}, 
                goto='__end__'
            )

        user_request = self.context_query or self.original_query

        # ğŸ”¹ ì‹¤í–‰í•  ML í”„ë¡œì„¸ìŠ¤ ëª©ë¡ (ê³ ì •ëœ ìˆœì„œë¡œ ì§„í–‰)
        ml_process_steps = [
            "PROMPT_ML_SCALING",
            "PROMPT_ML_IMBALANCE_HANDLING",
            "PROMPT_ML_MODEL_SELECTION",
            "PROMPT_ML_HYPERPARAMETER_TUNING",
            "PROMPT_ML_MODEL_EVALUATION",
            "PROMPT_ML_FEATURE_IMPORTANCE"
        ]
        
        # ë§ˆíŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì ìš©)
        mart_info = self._get_mart_info()

        # ğŸ”¹ ì‹¤í–‰í•  ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ì ìš©
        generated_code_list = []
        for prompt in ml_process_steps:
            prompt_text = globals().get(prompt, None)  # ë¬¸ìì—´ì„ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ë¡œ ë³€í™˜
            if not prompt_text:
                print(f"âš  {prompt} í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue  # í•´ë‹¹ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            prompt_chain = ChatPromptTemplate.from_messages([
                ("system", prompt_text.format(mart_info=mart_info)),
                ("user", "user_request:\n{user_request}"),
            ])
            chain = prompt_chain | self.llm
            response = chain.invoke({
                "user_request": user_request,
            })
            generated_code_list.append(response.content)

        # ğŸ”¹ Python ì½”ë“œ ë¸”ë¡ë§Œ ì¶”ì¶œ
        list_code = []
        for code in generated_code_list:
            extracted_code = self._extract_code_from_llm_response(code)
            list_code.append(extracted_code)

        if not list_code:
            print("âš  ì½”ë“œ ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return Command(update={
                "generated_code": "# ì˜¤ë¥˜: ì½”ë“œ ë¸”ë¡ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                }, goto =  "__end__")

        tmp_code = "\n\n".join(list_code).strip()

        # ğŸ”¹ ì½”ë“œê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤í–‰ ì¤‘ì§€
        if not tmp_code:
            print("âš  ìƒì„±ëœ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return Command(
                update={"generated_code": "# ì˜¤ë¥˜: ìƒì„±ëœ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."}, 
                goto=END
            )

        # ğŸ”¹ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ (LLMì„ ì´ìš©í•˜ì—¬ ì½”ë“œ í†µí•©)
        prompt_chain = ChatPromptTemplate.from_messages([
            ("system", PROMPT_MERGE_GENERAL_ML_CODE.replace("{", "{{").replace("}", "}}")),  # âœ… ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
            ("user", "ì‚¬ìš©ìê°€ ì œê³µí•œ ì½”ë“œ ë¸”ë¡:\n{merged_code}")
        ])
        chain = prompt_chain | self.llm
        response = chain.invoke({"merged_code": tmp_code})  # âœ… KeyError í•´ê²°
        # ğŸ”¹ í†µí•©ëœ ìµœì¢… ML ì½”ë“œ ì €ì¥
        final_code = response.content.strip()

        print(f"ğŸ¤– ìƒì„±ëœ ML ì½”ë“œ:\n{final_code[:500]}...\n")  # ì¼ë¶€ë§Œ ì¶œë ¥
        return Command(update={
            "generated_code": final_code,
            "regenerated_code": None,  # ì´ˆê¸°í™”
            "validated_code": None     # ì´ˆê¸°í™”
        }, goto="Execute_Sample")

    #########################################################
    # âœ… Generate_Code ë…¸ë“œ
    # -> Execute_Sample / END
    #########################################################
    def generate_python_code(self, state):
        """
        ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ Python ì½”ë“œ ìƒì„±
        IF í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŒ -> END ë…¸ë“œë¡œ ì´ë™
        ELSE ë°ì´í„°í”„ë ˆì„ ì •ë³´ ìƒì„± ë° ì½”ë“œ ìƒì„± -> Execute_Sample ë…¸ë“œë¡œ ì´ë™
        """
        print("="*100)
        print("ğŸ¤– ì½”ë“œ ìƒì„± ë‹¨ê³„:")
        
        # í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.active_marts:
            print("âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.")
            return Command(
                update={"error_message": "âŒ í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”."}, 
                goto='__end__'
            )
        
        user_request = self.context_query or self.original_query
        
        # ë§ˆíŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì ìš©)
        mart_info = self._get_mart_info()

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (format í™œìš©)
        prompt_text = PROMPT_GENERATE_CODE.format(mart_info=mart_info)

        prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_text),
            ("user", "\nuser_request:\n{user_request}")
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "user_request": user_request,
        })
        print(f"ğŸ¤– ìƒì„±ëœ ì½”ë“œ:\n{response.content}\n")
        return Command(update={
            "generated_code": response.content,
            "regenerated_code": None,  # ì´ˆê¸°í™”
            "validated_code": None     # ì´ˆê¸°í™”
        }, goto="Execute_Sample")
    
    #########################################################
    # âœ… Execute_Sample ë…¸ë“œ
    # -> Execute_Full / Regenerate_Code / END
    #########################################################
    def execute_sample_code(self, state):
        """ìƒ˜í”Œ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ Python ì½”ë“œ ì‹¤í–‰"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ§ª ìƒ˜í”Œ ì‹¤í–‰ ë‹¨ê³„")
        
        # ê° ë§ˆíŠ¸ë³„ë¡œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_marts = {}
        for mart_name, df in self.active_marts.items():
            sample_size = min(50, len(df))
            sample_marts[mart_name] = df.sample(n=sample_size)
            print(f"ğŸ§ª {mart_name}: {sample_size}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
    
        # print(f"ğŸ§ª ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ ì§ì „ ê¸€ë¡œë²Œ í‚¤ í™•ì¸(ì ‘ê·¼ ê°€ëŠ¥ ë°ì´í„°í”„ë ˆì„) \n {globals().keys()} ")

        try:
            # ì¬ìƒì„±ëœ ì½”ë“œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì´ˆê¸° ìƒì„± ì½”ë“œ ì‚¬ìš©
            code_to_execute = self._extract_code_from_llm_response(
                state.get("regenerated_code") or state["generated_code"]
            )
            
            # ì‹¤í–‰ í™˜ê²½ì— ìƒ˜í”Œ ë°ì´í„°í”„ë ˆì„ ì¶”ê°€
            exec_globals = {}

            # ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„(df) ìë™ í• ë‹¹
            if len(sample_marts) == 1:
                exec_globals["df"] = list(sample_marts.values())[0]  # ìœ ì¼í•œ ë§ˆíŠ¸ë¥¼ dfë¡œ í• ë‹¹
            elif len(sample_marts) > 1:
                exec_globals["df"] = list(sample_marts.values())[0]  # ì²« ë²ˆì§¸ ë§ˆíŠ¸ë¥¼ ê¸°ë³¸ dfë¡œ ì„¤ì •

            # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ exec_globalsì— ì¶”ê°€
            exec_globals.update(sample_marts)

            # print(f"ğŸ”¹ ì‹¤í–‰ í™˜ê²½ì— ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ ëª©ë¡: {list(exec_globals.keys())}")

            # ì¶”ì¶œëœ ì½”ë“œ ì‹¤í–‰
            self._execute_code_with_capture(code_to_execute, exec_globals, is_sample=True)
            
            print(f"âœ… ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ ì„±ê³µ")
            self.retry_count = 0  # ì„±ê³µ ì‹œ ì¹´ìš´í„° ì´ˆê¸°í™”
            return Command(update={
                "error_message": None
            }, goto="Execute_Full")

        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨")
            print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            print(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            print(traceback.format_exc())
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            self.retry_count += 1
            if self.retry_count >= MAX_RETRIES:
                print("âš ï¸ ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ 3íšŒ ì‹¤íŒ¨ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
                return Command(update={"error_message": error_details}, goto="__end__")
            return Command(update={"error_message": error_details}, goto="Regenerate_Code")

    #########################################################
    # âœ… Regenerate_Code ë…¸ë“œ
    # -> Execute_Full / Execute_Sample / END
    #########################################################
    def regenerate_code(self, state):
        """ì½”ë“œ ì‹¤í–‰ ì˜¤ë¥˜ ë°œìƒ ì‹œ LLMì„ í™œìš©í•˜ì—¬ ì½”ë“œ ì¬ìƒì„±"""
        from_full_execution = state.get("from_full_execution", False)  # í”Œë˜ê·¸ í™•ì¸

        if self.retry_count >= MAX_RETRIES:  # âœ… 3íšŒ ì´ˆê³¼ ì‹œ ì¢…ë£Œ
            return Command(goto=END)
        
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("âš’ï¸ ì½”ë“œ ì¬ìƒì„± ë‹¨ê³„")
        user_request = self.context_query or self.original_query
        error_message = state["error_message"]
        original_code = state["generated_code"]

        # í† í° ì´ˆê³¼ ì‹œ ì½”ë“œ ì¬ìƒì„±
        if state.get("from_token_limit", False):
            print(f"âš’ï¸ í† í° ì´ˆê³¼ ì‹œì˜ ì½”ë“œ ì¬ìƒì„± ì§„í–‰")
            prompt = ChatPromptTemplate.from_messages([
                    ("system", PROMPT_REGENERATE_CODE_WHEN_TOKEN_OVER),
                    ("user", "\nuser_request:\n{user_request}"),
            ])
        # ì¼ë°˜ ì½”ë“œ ì¬ìƒì„±
        else:
            prompt = ChatPromptTemplate.from_messages([
                        ("system", PROMPT_REGENERATE_CODE),
                        ("user", "\nuser_request:\n{user_request}"),
                        ("user", "\noriginal_code:\n{original_code}"),
                        ("user", "\nerror_message:\n{error_message}"),
                ])
        
        chain = prompt | self.llm
        
        # ì½”ë“œ ì¬ìƒì„±
        response = chain.invoke({
            "user_request": user_request,
            "original_code": original_code,
            "error_message": error_message
        })
        print(f"âš’ï¸ ì¬ìƒì„±ëœ ì½”ë“œ:\n{response.content}\n")
        next_step = "Execute_Full" if from_full_execution else "Execute_Sample"
        
        return Command(update={
            "regenerated_code": response.content,  # ì¬ìƒì„±ëœ ì½”ë“œ ì €ì¥
            "validated_code": None,  # validated_code ì´ˆê¸°í™”
            "from_full_execution": from_full_execution
        }, goto=next_step)

    #########################################################
    # âœ… Execute_Full ë…¸ë“œ
    # -> Save_Data / Regenerate_Code 
    #########################################################
    def execute_full_data(self, state):
        """ì „ì²´ ë°ì´í„°ë¡œ Python ì½”ë“œ ì‹¤í–‰"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ”„ ì „ì²´ ë°ì´í„° ì‹¤í–‰ ë‹¨ê³„")

        # ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì„¤ì •
        full_marts = self.active_marts  # ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©

        # ì‹¤í–‰ í™˜ê²½ì— ì „ì²´ ë°ì´í„°í”„ë ˆì„ ì¶”ê°€
        exec_globals = {}

        # ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„(df) ìë™ í• ë‹¹
        if len(full_marts) == 1:
            exec_globals["df"] = list(full_marts.values())[0]  # ìœ ì¼í•œ ë§ˆíŠ¸ë¥¼ dfë¡œ í• ë‹¹
        elif len(full_marts) > 1:
            exec_globals["df"] = list(full_marts.values())[0]  # ì²« ë²ˆì§¸ ë§ˆíŠ¸ë¥¼ ê¸°ë³¸ dfë¡œ ì„¤ì •

        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì„ exec_globalsì— ì¶”ê°€
        exec_globals.update(full_marts)

        print(f"ğŸ”„ ì „ì²´ ë°ì´í„° ì‹¤í–‰ í™˜ê²½ì— ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„ ëª©ë¡: {list(exec_globals.keys())}")

        # LLM ìƒì„± ì½”ë“œì—ì„œ ```python ë¸”ë¡ ì œê±°
        code_to_execute = self._extract_code_from_llm_response(
            state.get("regenerated_code") or state["generated_code"]
        )
        try:
            # ì „ì²´ ì½”ë“œ ì‹¤í–‰
            output, analytic_results = self._execute_code_with_capture(code_to_execute, exec_globals, is_sample=False)
            token_count = self._calculate_tokens(str(analytic_results))
            
            print(f"ğŸ”„ ê²°ê³¼ ë°ì´í„° í† í° ìˆ˜: {token_count}")
            
            if token_count > TOKEN_LIMIT:
                print(f"âš ï¸ í† í° ìˆ˜ ì´ˆê³¼: {token_count} > {TOKEN_LIMIT}")
                self.retry_count += 1
                return Command(update={
                    "error_message": f"ê²°ê³¼ ë°ì´í„° analytic_resultsì˜ ì ì • í† í° ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. analytic_resultsì— Raw ë°ì´í„° í˜¹ì€ ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì ì¬ë¥¼ í”¼í•´ì£¼ì„¸ìš”: {token_count} > {TOKEN_LIMIT}",
                    "from_full_execution": True,  # í”Œë˜ê·¸ ì¶”ê°€
                    "from_token_limit": True
                }, goto="Regenerate_Code")
            
            print(f"ğŸ”„ ì „ì²´ ë°ì´í„° ì‹¤í–‰ ì„±ê³µ")
            print(f'ğŸ”„ analytic_results\n {analytic_results}')
            # print(f'ğŸ”„ : output\n {output}')

            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
            if analytic_results is not None:
                unique_id = datetime.now().strftime("%Y%m%d%H%M%S")
                # ì „ì²´ ì‹¤í–‰ ì„±ê³µ ì‹œ validated_code ì„¤ì •
                current_code = state.get("regenerated_code") or state["generated_code"]
                return Command(update={
                    "analytic_result": analytic_results,
                    "execution_output": output,
                    "data_id": unique_id,
                    "validated_code": current_code  # ì„±ê³µí•œ ì½”ë“œë¥¼ validated_codeë¡œ ì €ì¥
                }, goto="Save_Data")
            # ë¶„ì„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            else:
                print("âš ï¸ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.retry_count += 1
                return Command(update={
                    "error_message": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "execution_output": output,
                    "from_full_execution": True  # í”Œë˜ê·¸ ì¶”ê°€
                }, goto="Regenerate_Code")

        except Exception as e:
            print(f"âŒ ì „ì²´ ë°ì´í„° ì‹¤í–‰ ì‹¤íŒ¨\n {code_to_execute}")
            print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            print(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
            print(f"ì—ëŸ¬ ë°œìƒ ìœ„ì¹˜:")
            print(traceback.format_exc())
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "traceback": traceback.format_exc()
            }
            self.retry_count += 1
            return Command(update={
                "error_message": error_details,
                "from_full_execution": True  # í”Œë˜ê·¸ ì¶”ê°€
            }, goto="Regenerate_Code")

    #########################################################
    # âœ… Save_Data ë…¸ë“œ
    # -> Insight_Builder
    #########################################################
    def save_data(self, state):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥ (ID ë¶€ì—¬)"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ“‚ ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ë‹¨ê³„")
        # data_idê°€ ì—†ëŠ” ê²½ìš° ìƒì„±
        data_id = state.get("data_id", datetime.now().strftime("%Y%m%d%H%M%S"))
        analytic_result = state["analytic_result"]
        execution_output = state["execution_output"]
        # ë¶„ì„ ê²°ê³¼ì™€ ì‹¤í–‰ ì¶œë ¥ì„ í•¨ê»˜ ì €ì¥
        save_data = {
            'analytic_result': analytic_result,
            'execution_output': execution_output
        }

        # ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
        os.makedirs("../output", exist_ok=True)
        with open(f"../output/data_{data_id}.pkl", 'wb') as f:
            pickle.dump(save_data, f)

        print(f"ğŸ“‚ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ: ../output/data_{data_id}.pkl")
        return Command(update={"data_id": data_id}, goto="Insight_Builder")
    
    #########################################################
    # âœ… Insight_Builder ë…¸ë“œ
    # -> Chart_Builder / Report_Builder
    #########################################################
    def generate_insights(self, state):
        """ì €ì¥ëœ ë°ì´í„°ì—ì„œ ìë™ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë° ì°¨íŠ¸ í•„ìš” ì—¬ë¶€ ê²°ì •"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ’¡ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë‹¨ê³„")
        dict_result = state["analytic_result"]
        user_question = state["messages"][0].content

        # âœ… ì§‘ê³„ ë°ì´í„°ë©´ ì „ì²´ ë°ì´í„° ì „ë‹¬
        string_of_result = str(dict_result)

        ############################################################
        # 1. ì¸ì‚¬ì´íŠ¸ ìƒì„±
        ############################################################
        prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_INSIGHT_BUILDER),
            ("user", "user_question:\n{user_question}\n\n"),
            ("user", "analytic_result:\n{analytic_result}\n\n")
        ])

        chain = prompt | self.llm
        insight_response = chain.invoke({
            "user_question": user_question,
            "analytic_result": string_of_result
        })

        print(f"ğŸ’¡ ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸\n{insight_response.content}")
        
        ############################################################
        # 2. ì°¨íŠ¸ í•„ìš” ì—¬ë¶€ ê²°ì •
        ############################################################
        prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_CHART_NEEDED),
            ("user", "user_question:\n{user_question}\n\n"),
            ("user", "analytic_result:\n{analytic_result}\n\n"),
            ("user", "insight:\n{insight}\n\n")
        ])
        
        # ì°¨íŠ¸ í™œìš© ì—¬ë¶€ 'yes' ë˜ëŠ” 'no' ë°˜í™˜
        chart_decision_messages = prompt | self.llm
        chart_needed = chart_decision_messages.invoke({
            "user_question": user_question,
            "analytic_result": string_of_result,
            "insight": insight_response.content
        }).content.strip().lower()
        print(f"ğŸ’¡ ì°¨íŠ¸ í•„ìš” ì—¬ë¶€ (yes/no): {chart_needed}")
        
        # ì°¨íŠ¸ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        next_step = "Chart_Builder" if chart_needed == "yes" else "Report_Builder"
        
        return Command(update={
            "insights": insight_response.content,
            "chart_needed": chart_needed == "yes"
        }, goto=next_step)  # Supervisor ëŒ€ì‹  ì ì ˆí•œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
        
    #########################################################
    # âœ… Chart_Builder ë…¸ë“œ
    # -> Report_Builder / Regenerate_Chart
    #########################################################
    def generate_chart(self, state):
        """ì°¨íŠ¸ ìƒì„± ë¡œì§ (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ“Š ì°¨íŠ¸ ìƒì„± ë‹¨ê³„")

        if self.retry_count >= MAX_RETRIES:
            print("âš ï¸ ì°¨íŠ¸ ìƒì„± 3íšŒ ì‹¤íŒ¨. ì°¨íŠ¸ ì—†ì´ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            self.retry_count = 0  # ì¹´ìš´í„° ì´ˆê¸°í™”
            return Command(update={"chart_filename": None}, goto="Report_Builder")
        
        try:
            analytic_result = state.get("analytic_result", {})
            string_of_result = str(analytic_result)
            insights = state.get('insights', 'ì¸ì‚¬ì´íŠ¸ ì—†ìŒ')
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", PROMPT_CHART_GENERATOR),
                ("user", """
Here are the analytic_result and insights to visualize:

Analysis Results Summary:
{analytic_result}

Key Insights:
{insights}

Please create an appropriate visualization that supports these insights.
Do not hardcode any values - use the analytic_result dictionary directly.
                """)            
            ])

            chain = prompt | self.llm
            chart_code = chain.invoke({
                "analytic_result": string_of_result,
                "insights": insights
            }).content
            
            print(f"ğŸ’¡ ìƒì„±ëœ ì°¨íŠ¸ ì½”ë“œ\n{chart_code}")
            
            # âœ… ì°¨íŠ¸ ì½”ë“œ ë¸”ë¡ì´ ìˆëŠ” ê²½ìš° ì½”ë“œ ì¶”ì¶œ
            extracted_code = self._extract_code_from_llm_response(chart_code)
            
            # ğŸ”¹ ê¸°ì¡´ì— LLMì´ ìƒì„±í•œ ì½”ë“œì—ì„œ `plt.show()` ì œê±°
            extracted_code = extracted_code.replace("plt.show()", "").strip()

            # ğŸ”¹ ì°¨íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("../img", exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            filename = f"../img/chart_{timestamp}.png"

            # ğŸ”¹ `plt.savefig()`ë¥¼ ë¨¼ì € ì‹¤í–‰í•œ í›„ `plt.show()` ì¶”ê°€
            extracted_code += f"\nplt.savefig('{filename}', dpi=300)\nplt.show()"

            # âœ… ë””ë²„ê¹…ìš© ì¶œë ¥ (ìƒì„±ëœ ì½”ë“œ í™•ì¸)
            # print(f"ğŸ“Š ìƒì„±ëœ ì°¨íŠ¸ ì½”ë“œ\n{extracted_code}")
            
            # ì‹¤í–‰ í™˜ê²½ ì„¤ì •
            exec_globals = {
                'plt': plt,
                'np': np,
                'sns': sns,
                'analytic_result': analytic_result,
            }
            
            # ì½”ë“œ ì‹¤í–‰
            exec(extracted_code, exec_globals)
            plt.close()

            print(f"âœ… ì°¨íŠ¸ ìƒì„± ì„±ê³µ: {filename}")
            self.retry_count = 0  # ì„±ê³µ ì‹œ ì¹´ìš´í„° ì´ˆê¸°í™”
            return Command(update={
                "chart_filename": filename,
                "chart_error": None
            }, goto="Report_Builder")

        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ì½”ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            plt.close()
            error_info = {
                "error_message": str(e),
                "previous_code": extracted_code,
                "traceback": traceback.format_exc()
            }
            # âŒ ì‹¤íŒ¨ ì‹œ Regenerate_Chartë¡œ
            self.retry_count += 1
            return Command(
                update={
                    "chart_filename": None,
                    "chart_error": error_info
                },
                goto="Regenerate_Chart"
            )

    #########################################################
    # âœ… Regenerate_Chart ë…¸ë“œ
    # -> Report_Builder / Regenerate_Chart
    #########################################################
    def regenerate_chart(self, state):
        """ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ ì¬ìƒì„±"""
        print("="*100)
        print("ğŸ”„ ì°¨íŠ¸ ì¬ìƒì„± ë‹¨ê³„")
        
        dict_result = state["analytic_result"]
        string_of_result = str(dict_result)
        insights = state.get('insights', 'ì¸ì‚¬ì´íŠ¸ ì—†ìŒ')
        previous_error = state.get("chart_error", {})

        if self.retry_count >= MAX_RETRIES:
            print("âš ï¸ ì°¨íŠ¸ ì¬ìƒì„± 3íšŒ ì‹¤íŒ¨. ì°¨íŠ¸ ì—†ì´ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
            self.retry_count = 0  # ì¹´ìš´í„° ì´ˆê¸°í™”
            return Command(update={
                "chart_filename": None,
                "chart_error": None
            }, goto="Report_Builder")

        prompt = ChatPromptTemplate.from_messages([
            ("system", "Python ì½”ë“œì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”."),  # ì°¨íŠ¸ ì¬ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
            ("user", """
    ì´ì „ ì°¨íŠ¸ ìƒì„± ì‹œë„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:
    ì—ëŸ¬ ë©”ì‹œì§€: {error_message}

    ì´ì „ ì½”ë“œ:
    {previous_code}

    ì „ì²´ ì—ëŸ¬ ë‚´ìš©:
    {error_traceback}

    ë¶„ì„í•  ë°ì´í„°:
    {analytic_result}

    ì¸ì‚¬ì´íŠ¸:
    {insights}

    ìœ„ì˜ ì—ëŸ¬ë¥¼ í•´ê²°í•œ ìƒˆë¡œìš´ ì°¨íŠ¸ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
            """)
        ])

        chain = prompt | self.llm
        chart_code = chain.invoke({
            "error_message": previous_error.get("error_message", "ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬"),
            "previous_code": previous_error.get("previous_code", "ì´ì „ ì½”ë“œ ì—†ìŒ"),
            "error_traceback": previous_error.get("traceback", "íŠ¸ë ˆì´ìŠ¤ë°± ì—†ìŒ"),
            "analytic_result": string_of_result,
            "insights": insights
        }).content

        extracted_code = self._extract_code_from_llm_response(chart_code)

        # âœ… ìœ íš¨í•œ Python ì½”ë“œ ë¸”ë¡ì´ ì—†ëŠ” ê²½ìš° ì¬ì‹œë„
        if not extracted_code:
            print("ğŸ“Š [regenerate_chart] ìœ íš¨í•œ Python ì½”ë“œ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            error_info = {
                "error_message": "ìœ íš¨í•œ Python ì½”ë“œ ë¸”ë¡ì´ ì—†ìŠµë‹ˆë‹¤",
                "previous_code": chart_code
            }
            self.retry_count += 1
            return Command(update={
                "retry_count": self.retry_count + 1,
                "chart_error": error_info
            }, goto="Regenerate_Chart")

        # âœ… ì°¨íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("../img", exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        filename = f"../img/chart_{timestamp}.png"

        # ğŸ”¹ `plt.show()` ì œê±°
        extracted_code = extracted_code.replace("plt.show()", "").strip()

        # ğŸ”¹ `plt.savefig()` ì¶”ê°€
        extracted_code += f"\nplt.savefig('{filename}', dpi=300)\nplt.show()"
        
        print(f"ğŸ“Š ì‹¤í–‰í•  ì°¨íŠ¸ ì½”ë“œ:\n{extracted_code}")

        try:
            exec(extracted_code, globals())
            print(f"âœ… ì°¨íŠ¸ ì¬ìƒì„± ì„±ê³µ: {filename}")
            plt.close()
            self.retry_count = 0  # ì„±ê³µ ì‹œ ì¹´ìš´í„° ì´ˆê¸°í™”
            return Command(update={
                "chart_filename": filename,
                "chart_error": None
            }, goto="Report_Builder")

        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            plt.close()
            error_info = {"error_message": str(e),"previous_code": extracted_code,"traceback": traceback.format_exc()}
            self.retry_count += 1
            return Command(update={
                "chart_filename": None,
                "chart_error": error_info
            }, goto="Regenerate_Chart")

    #########################################################
    # âœ… Report_Builder ë…¸ë“œ
    # -> END
    #########################################################
    def generate_report(self, state):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print("ğŸ“‘ ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„")
        dict_result = state["analytic_result"]
        string_of_result = str(dict_result)
        insights = state.get('insights', 'ì¸ì‚¬ì´íŠ¸ ì—†ìŒ')
        user_request = self.original_query
        prompt = ChatPromptTemplate.from_messages([
            ("system", PROMPT_REPORT_GENERATOR),
            ("user", "1. ë¶„ì„ ê²°ê³¼ ë°ì´í„°\n{analytic_result}\n\n"),
            ("user", "2. ì‚¬ìš©ì ìš”ì²­\n{user_request}\n\n"),
            ("user", "3. ë„ì¶œëœ ì¸ì‚¬ì´íŠ¸\n{insights}\n\n"),
        ])

        chain = prompt | self.llm
        response = chain.invoke({
            "user_request": user_request,
            "analytic_result": string_of_result,
            "insights": insights,
        })
        print("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
        print(f"{response.content}")
        return Command(update={
            "report": response.content, 
            "error_message": None
        }, goto=END)
    

    ##################################################################################################################
    # ë¼ìš°í„° ëª¨ìŒ
    ##################################################################################################################
    def route_after_generate_code(self, state: State):
        """ì½”ë“œ ìƒì„± í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
        print("â¡ï¸ [route_after_generate_code] ì½”ë“œ ìƒì„± í›„ ê²½ë¡œ ê²°ì •")

        if state.get("generated_code"):
            print("â¡ï¸ [route_after_generate_code] ìƒ˜í”Œ ì‹¤í–‰ ì§„í–‰")
            return "Execute_Sample"
        else:
            print("â¡ï¸ [route_after_generate_code] ë§ˆíŠ¸ í™œì„±í™” í•„ìš” -> [í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ]")
            return END


    def route_after_sample(self, state: State):
        """ìƒ˜í”Œ ì‹¤í–‰ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
        print("â¡ï¸ [route_after_sample] ìƒ˜í”Œ ì‹¤í–‰ í›„ ê²½ë¡œ ê²°ì •")
        
        if not self.active_marts or self.active_marts is None:
            print("â¡ï¸ [route_after_sample] í™œì„±í™”ëœ ë§ˆíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë§ˆíŠ¸ë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”.")
            return END
        
        if not state.get("error_message"):  # ì—ëŸ¬ê°€ ì—†ìœ¼ë©´
            print("â¡ï¸ [route_after_sample] ì „ì²´ ë°ì´í„° ì‹¤í–‰ ì§„í–‰")
            return "Execute_Full"
        else:
            if self.retry_count >= MAX_RETRIES:
                print("âš ï¸ ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ 3íšŒ ì‹¤íŒ¨ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
                self.retry_count = 0
                return END
            print(f"âš ï¸ ìƒ˜í”Œ ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨ â†’ ì½”ë“œ ì¬ìƒì„± í•„ìš” | ì¬ì‹œë„ íšŸìˆ˜: {self.retry_count}")
            return "Regenerate_Code"


    def route_after_insights(self, state: State) -> str:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„± í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
        print("="*100)  # êµ¬ë¶„ì„  ì¶”ê°€
        print(f"â¡ï¸ [route_after_insights] ì¸ì‚¬ì´íŠ¸ ìƒì„± í›„ ê²½ë¡œ ê²°ì •(ì°¨íŠ¸ or ë³´ê³ ì„œ)")
        
        if state.get("chart_needed", False):
            print("â¡ï¸ [route_after_insights] ì°¨íŠ¸ ìƒì„± ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤")
            return "Chart_Builder"
        print("â¡ï¸ [route_after_insights] ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤")
        return "Report_Builder"
    
    def route_after_chart(self, state: State) -> str:
        """ì°¨íŠ¸ ìƒì„± í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
        print(f"â¡ï¸ [route_after_chart] ì°¨íŠ¸ ìƒì„± í›„ ê²½ë¡œ ê²°ì •(ì°¨íŠ¸ ì¬ìƒì„± or ë³´ê³ ì„œ)")

        if state.get("chart_filename"):
            print("â¡ï¸ [route_after_chart] ì°¨íŠ¸ ìƒì„± ì„±ê³µ â†’ ë¦¬í¬íŠ¸ ìƒì„± ë‹¨ê³„ë¡œ ì§„í–‰")
            return "Report_Builder"
        
        if self.retry_count >= MAX_RETRIES:
            print("âš ï¸ ì°¨íŠ¸ ìƒì„± 3íšŒ ì‹¤íŒ¨ â†’ ì°¨íŠ¸ ì—†ì´ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì§„í–‰")
            self.retry_count = 0  # ì¹´ìš´í„° ì´ˆê¸°í™”
            return "Report_Builder"
        
        print(f"â¡ï¸ ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨ â†’ ì¬ìƒì„± ì‹œë„ (Regenerate_Chart) ({self.retry_count + 1}/3)")
        return "Regenerate_Chart"


    def route_after_regenerate(self, state: State) -> str:
        """ì½”ë“œ ì¬ìƒì„± í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
        from_full_execution = state.get("from_full_execution", False)
        if self.retry_count >= MAX_RETRIES:
            print("âš ï¸ ì½”ë“œ ì¬ìƒì„± 3íšŒ ì‹¤íŒ¨ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
            return END
        
        if from_full_execution:
            print("â¡ï¸ [route_after_regenerate] ì „ì²´ ë°ì´í„° ì‹¤í–‰ìœ¼ë¡œ ì§„í–‰")
            return "Execute_Full"
        else:
            print("â¡ï¸ [route_after_regenerate] ìƒ˜í”Œ ì‹¤í–‰ìœ¼ë¡œ ì§„í–‰")
            return "Execute_Sample"
        

    def route_after_full_execution(self, state: State) -> str:
        """ì „ì²´ ë°ì´í„° ì‹¤í–‰ í›„ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ëŠ” ë¼ìš°í„°
        
        Returns:
            str: ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œì˜ ì´ë¦„
        """
        print("â¡ï¸ [route_after_full_execution] ì „ì²´ ë°ì´í„° ì‹¤í–‰ í›„ ê²½ë¡œ ê²°ì •")
        
        if state.get("validated_code"):  # validated_codeê°€ ìˆìœ¼ë©´ ì„±ê³µ
            print("â¡ï¸ [route_after_full_execution] ë°ì´í„° ì €ì¥ ë‹¨ê³„ë¡œ ì§„í–‰")
            return "Save_Data"
        
        if self.retry_count >= MAX_RETRIES:
            print("âš ï¸ ì „ì²´ ë°ì´í„° ì‹¤í–‰ 3íšŒ ì‹¤íŒ¨ â†’ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
            return END
        
        print(f"âš ï¸ ì „ì²´ ë°ì´í„° ì‹¤í–‰ ì‹¤íŒ¨ â†’ ì½”ë“œ ì¬ìƒì„± í•„ìš” | ì¬ì‹œë„ íšŸìˆ˜: {self.retry_count}")
        return "Regenerate_Code"

    def route_after_report(self, state: State) -> str:
        """Report_Builder ì´í›„ EDA ë‹¨ê³„ë¥¼ ì¶”ê°€ ì‹¤í–‰í• ì§€ ê²°ì •"""
        selected_categories = state.get("selected_categories", [])  # âœ… ê¸°ë³¸ê°’ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
        eda_stage = state.get("eda_stage", 0)

        # âœ… ëª¨ë“  EDA ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆë‹¤ë©´ ENDë¡œ ì´ë™
        if eda_stage >= len(selected_categories):
            return END

        return "Eda_Generate_Code"

    ##################################################################################################################
    # í•¨ìˆ˜ ëª¨ìŒ
    ##################################################################################################################
    def set_active_mart(self, data_mart: Union[pd.DataFrame, Dict[str, pd.DataFrame]], mart_name: Union[str, List[str], None] = None) -> None:
        """ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ê³¼ ë§ˆíŠ¸ ì •ë³´ë¥¼ ì„¤ì •"""
        if isinstance(data_mart, pd.DataFrame):
            # ë‹¨ì¼ ë°ì´í„°í”„ë ˆì„ ì„¤ì •
            mart_key = mart_name if mart_name else "default_mart"
            self.active_marts = {mart_key: data_mart}
        elif isinstance(data_mart, dict):
            # ë‹¤ì¤‘ ë°ì´í„°í”„ë ˆì„ ì„¤ì •
            self.active_marts = data_mart
        else:
            raise TypeError("ì…ë ¥ëœ ë°ì´í„°ê°€ pandas DataFrame ë˜ëŠ” DataFrame ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")

        # ë§ˆíŠ¸ ì •ë³´ ì„¤ì • (ì—‘ì…€ íŒŒì¼ì˜ sheetì—ì„œ ê°€ì ¸ì˜´)
        mart_info_list = []
        for mart_key in self.active_marts.keys():
            if mart_key in self.mart_info_df:
                mart_info_list.append(f"## {mart_key} ë§ˆíŠ¸ ì •ë³´\n{self.mart_info_df[mart_key].to_markdown()}")
        
        self.mart_info = "\n\n".join(mart_info_list) if mart_info_list else None

        # ë°ì´í„°í”„ë ˆì„ ê°œìˆ˜ ë° ì •ë³´ ì¶œë ¥
        print(f"ğŸ”¹ {len(self.active_marts)}ê°œì˜ ë°ì´í„°í”„ë ˆì„ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for name, df in self.active_marts.items():
            print(f"ğŸ”¹ ë°ì´í„°ë§ˆíŠ¸ ì´ë¦„: {name}")
            print(f"ğŸ”¹ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
            if self.mart_info and name in self.mart_info_df:
                print(f"ğŸ”¹ ë§ˆíŠ¸ ì •ë³´ ë¡œë“œë¨")
    
    # ìƒì„±í˜• AIê°€ ìƒì„±í•œ ì½”ë“œë¥¼ ì „ì²´ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¶œë ¥ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    def _execute_code_with_capture(self, code, exec_globals, is_sample=False):
        
        # í‘œì¤€ ì¶œë ¥ì„ ê°€ë¡œì±„ê¸° ìœ„í•´ StringIO ì‚¬ìš©
        captured_output = io.StringIO()
        original_stdout = sys.stdout  # ì›ë˜ í‘œì¤€ ì¶œë ¥ ì €ì¥

        # âœ… ì‹¤í–‰ ì „, exec_globals ì´ˆê¸°í™” (ì´ì „ ê°’ ìœ ì§€ ë°©ì§€)
        safe_locals = {}

        try:
            sys.stdout = captured_output  # í‘œì¤€ ì¶œë ¥ ë³€ê²½
            exec(code, exec_globals, safe_locals)  # **ì œí•œëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì‹¤í–‰**
            sys.stdout = original_stdout # í‘œì¤€ ì¶œë ¥ì„ ì›ë˜ëŒ€ë¡œ ë³µì›
            
            print(f"ğŸ”„ [_execute_code_with_capture] ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ ê°ì²´ : {safe_locals.keys()}")

            # ë¶„ì„ ê²°ê³¼ ì´ˆê¸°í™”
            results = None
            analytic_results = None
            
            # ì „ì²´ ë°ì´í„° ì‹¤í–‰ ì‹œ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
            if not is_sample:
                if "result_df" in safe_locals:
                    results = safe_locals["result_df"]
                elif "analytic_results" in safe_locals:
                    results = safe_locals["analytic_results"]
                
                # ê²°ê³¼ íƒ€ì…ì— ë”°ë¥¸ í‘œì¤€í™” ì²˜ë¦¬
                if results is not None:
                    if isinstance(results, pd.DataFrame):
                        # DataFrameì„ dictionaryë¡œ ë³€í™˜
                        analytic_results = results.to_dict('records') if not results.empty else {}
                    elif isinstance(results, dict):
                        # DictionaryëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        analytic_results = results
                    elif isinstance(results, list):
                        # ListëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        analytic_results = results
                    else:
                        # ê¸°íƒ€ íƒ€ì…ì€ dictionaryë¡œ ë³€í™˜
                        analytic_results = {"result": str(results)}
            
            # ì¶œë ¥ ë° ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return captured_output.getvalue(), analytic_results
            
        except Exception as e:
            captured_output.write(f"Error: {str(e)}\n")  # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
            sys.stdout = original_stdout
            raise e

    def _calculate_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì†Œë“œ
        
        Args:
            text (str): í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
            
        Returns:
            int: í† í° ìˆ˜
        """
        try:
            enc = tiktoken.get_encoding("cl100k_base")
            tokens = enc.encode(text)
            return len(tokens)
        except Exception as e:
            print(f"âš ï¸ í† í° ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return 0
        
    def _extract_code_from_llm_response(self, response: str) -> str:
        """LLM ì‘ë‹µì—ì„œ ì½”ë“œ ë¸”ë¡ì„ ì¶”ì¶œí•˜ëŠ” ë©”ì†Œë“œ
        
        Args:
            response (str): LLMì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            str: ì¶”ì¶œëœ ì½”ë“œ (ì½”ë“œ ë¸”ë¡ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜)
        """
        try:
            if "```python" in response:
                return response.split("```python")[1].split("```")[0].strip()
            elif "```" in response:
                return response.split("```")[1].strip()
            return response.strip()
        except Exception as e:
            print(f"âš ï¸ ì½”ë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return response.strip()

   
    def map_to_eda_category(self, user_request):
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ EDA ë‹¨ê³„ë¥¼ ì„ íƒ
        - 1ì°¨ì ìœ¼ë¡œ ì‚¬ì „ ì •ì˜ëœ ë™ì˜ì–´ ë§¤í•‘ì„ ì‚¬ìš©
        - 2ì°¨ì ìœ¼ë¡œ LLMì„ í™œìš©í•˜ì—¬ ì˜ë¯¸ë¥¼ ì¶”ë¡ 
        """
        # ì‚¬ì „ ì •ì˜ëœ ë™ì˜ì–´ ë§¤í•‘
        eda_synonyms = {
            "ê¸°ë³¸ ì •ë³´ ë¶„ì„": ["ê¸°ë³¸ ì •ë³´", "ë°ì´í„° ì •ë³´", "ë°ì´í„°ì…‹ ê°œìš”"],
            "ê¸°ì´ˆ í†µê³„ ë¶„ì„": ["ê¸°ì´ˆ í†µê³„", "í†µê³„ ë¶„ì„", "ë°ì´í„° ë¶„í¬"],
            "ê²°ì¸¡ì¹˜ ì²˜ë¦¬": ["ê²°ì¸¡ì¹˜", "Null ê°’", "ëˆ„ë½ëœ ê°’", "NaN", "ë¹ ì§„ ë°ì´í„°", "ì†Œì‹¤ëœ ê°’"],
            "ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„": ["ë³€ìˆ˜ ê´€ê³„", "ìƒê´€ê´€ê³„ ë¶„ì„", "íŠ¹ì„± ê´€ê³„"],
            "ì´ìƒì¹˜ íƒì§€": ["ì´ìƒì¹˜", "ì´ìƒê°’", "Outlier", "ë¹„ì •ìƒ ë°ì´í„°", "ì´ìƒí•œ ë°ì´í„°"]
        }
        
        # 1ï¸âƒ£ ì‚¬ì „ ì •ì˜ëœ í‚¤ì›Œë“œ ë§¤ì¹­
        selected_keywords = process.extract(user_request, sum(eda_synonyms.values(), []), limit=3)
        matched_categories = []
        for keyword, score in selected_keywords:
            if score > 80:
                for category, synonyms in eda_synonyms.items():
                    if keyword in synonyms:
                        matched_categories.append(category)
        
        # 2ï¸âƒ£ ë§Œì•½ ìœ ì‚¬ì–´ ë§¤ì¹­ì´ ì•ˆë˜ë©´ LLMì„ í™œìš©í•˜ì—¬ ë¶„ì„
        if not matched_categories:
            prompt = f"""
            ì‚¬ìš©ìê°€ EDA ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.  
            ì•„ë˜ ë¬¸ì¥ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¶„ì„ ë‹¨ê³„ë¥¼ ì •í™•íˆ íŒë³„í•˜ì„¸ìš”.  

            EDA ë¶„ì„ ë‹¨ê³„:
            1. ê¸°ë³¸ ì •ë³´ ë¶„ì„: ë°ì´í„° í¬ê¸°, íƒ€ì…, ê²°ì¸¡ê°’, ì¤‘ë³µ ì—¬ë¶€ í™•ì¸
            2. ê¸°ì´ˆ í†µê³„ ë¶„ì„: ë³€ìˆ˜ì˜ ë¶„í¬, ë³€ë™ì„±, ì •ê·œì„± ê²€ì •
            3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬: ê²°ì¸¡ê°’ ë¹„ìœ¨ í™•ì¸, ë³´ê°„ ë°©ë²• ì ìš© (ì˜ˆ: Null ê°’, NaN, ëˆ„ë½ëœ ê°’, ë¹ ì§„ ë°ì´í„°)
            4. ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„: ìƒê´€ê´€ê³„, ë‹¤ì¤‘ê³µì„ ì„±, ë²”ì£¼í˜• ë³€ìˆ˜ ê´€ê³„ ë¶„ì„
            5. ì´ìƒì¹˜ íƒì§€: ì´ìƒê°’ ê²€ì¶œ, ë°•ìŠ¤í”Œë¡¯ ì‹œê°í™” (ì˜ˆ: Outlier, ë¹„ì •ìƒ ë°ì´í„°)

            ì˜ˆì œ 1:
            ì…ë ¥: "ê²°ì¸¡ê°’ ë¶„ì„í•´ì¤˜"
            ì¶œë ¥: "ê²°ì¸¡ì¹˜ ì²˜ë¦¬"

            ì˜ˆì œ 2:
            ì…ë ¥: "ì´ìƒê°’ í™•ì¸ ë¶€íƒí•´"
            ì¶œë ¥: "ì´ìƒì¹˜ íƒì§€"

            ì‚¬ìš©ìì˜ ìš”ì²­ì´ ë‹¤ìŒê³¼ ê°™ì„ ë•Œ, ê°€ì¥ ì ì ˆí•œ EDA ë‹¨ê³„ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.  
            ìš”ì²­: "{user_request}"
            1. ë§Œì•½ EDA ì „ë°˜ì ì¸ ë‚´ìš©ì„ ë¬»ëŠ”ë‹¤ë©´ "ì „ì²´"ë¼ëŠ” ê°’ì„ ë°˜í™˜í•˜ì„¸ìš”.
            2. ë§Œì•½ EDA ì „ë°˜ì ì¸ ë‚´ìš©ì´ ì•„ë‹ˆë©° íŠ¹ì • ë‹¨ê³„ì— í•´ë‹¹ë˜ëŠ” ë‚´ìš©ì´ ì•„ë‹ˆë¼ë©´ "ê¸°íƒ€"ë¼ëŠ” ê°’ì„ ë°˜í™˜í•˜ì„¸ìš”.
            """
            
            llm_response = self.llm.invoke(prompt)  # LLMì„ í™œìš©í•˜ì—¬ ì˜ë¯¸ ë¶„ì„
            llm_response = llm_response.content
            print(f"ğŸ“Œ LLM ì¶”ë¡  ê²°ê³¼: {llm_response}")
            
            # LLMì´ ì¶”ì²œí•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ì¡´ ë§¤í•‘ê³¼ ë¹„êµí•˜ì—¬ íŒë‹¨
            for category in eda_synonyms.keys():
                if category in llm_response:
                    matched_categories.append(category)

            # ì ì ˆí•œ EDA ë‹¨ê³„ê°€ ì—†ì„ ê²½ìš° ì „ì²´ EDA í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰
            if not matched_categories:
                # matched_categories = list(eda_synonyms.keys())
                if "ì „ì²´" in llm_response:
                    matched_categories = ["ì „ì²´"]
                elif "ê¸°íƒ€" in llm_response:
                    matched_categories = ["ê¸°íƒ€"]
            print(f"ğŸ” ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬: {matched_categories}")
        
        return matched_categories
    

    def _get_mart_info(self) -> str:
        """ë°ì´í„°í”„ë ˆì„ì˜ ë§ˆíŠ¸ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        
        Returns:
            str: ë§ˆíŠ¸ ì •ë³´ ë¬¸ìì—´ (ë§ˆíŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜)
        """
        mart_info = ""
        if hasattr(self, 'active_marts') and self.active_marts:
            for mart_name in self.active_marts.keys():
                if mart_name in self.mart_info_df:
                    mart_info += f"\n- ë°ì´í„°í”„ë ˆì„ : {mart_name}ì˜ ì»¬ëŸ¼ ë° ì¸ìŠ¤í„´ìŠ¤ ì •ë³´ ##\n"
                    mart_info += self.mart_info_df[mart_name].to_markdown().replace("{", "{{").replace("}", "}}")  # ì´ìŠ¤ì¼€ì´í”„ ì ìš©
                    mart_info += "\n"
                else:
                    mart_info += f"\n## {mart_name} ë§ˆíŠ¸ ì •ë³´ ì—†ìŒ ##\n"
        else:
            mart_info = "ë°ì´í„°í”„ë ˆì„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        return mart_info
