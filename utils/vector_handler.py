# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import json
from glob import glob
from pathlib import Path

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore
from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader, 
    UnstructuredPowerPointLoader,
    JSONLoader,
    CSVLoader
)
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()

VECTOR_DB_SESSION_PATH = "./vector_db_session"
VECTOR_DB_BASE_PATH = "./vectordb"


def load_vectorstore():
    if os.path.exists(VECTOR_DB_BASE_PATH):
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        try:
            return FAISS.load_local(VECTOR_DB_BASE_PATH, embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    return None


def get_text(docs):
    """ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    doc_list = []
    for doc in docs:
        file_name = f'../documents/{doc.name}'
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())

        if file_name.endswith('.pdf'):
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.docx'):
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.json'):
            loader = JSONLoader(file_path=file_name, jq_schema='.[]', text_content=False)
            documents = loader.load()
        elif file_name.endswith('.csv'):
            loader = CSVLoader(file_name, encoding='utf-8')
            documents = loader.load()
        elif file_name.endswith('.xlsx'):
            df = pd.read_excel(file_name)
            documents = []
            for _, row in df.iterrows():
                content = f"í…Œì´ë¸”ëª…: {row['í…Œì´ë¸”ëª…']}, ì»¬ëŸ¼ëª…: {row['ì»¬ëŸ¼ëª…']}, ì»¬ëŸ¼í•œê¸€ëª…: {row['ì»¬ëŸ¼í•œê¸€ëª…']}, ì„¤ëª…: {row['ì»¬ëŸ¼ì„¤ëª…']}, ë°ì´í„° íƒ€ì…: {row['DATATYPE']}"
                documents.append(Document(page_content=content, metadata={"source": file_name}))
        elif file_name.endswith('.txt'):
            with open(file_name, "r", encoding="utf-8") as f:
                content = f.read()
            documents = [Document(page_content=content, metadata={"source": file_name})]
        
        doc_list.extend(documents)
    return doc_list

def get_text_chunks(text):
    """í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100
    )
    return text_splitter.split_documents(text)

def rebuild_vectorstore_without_document(doc_to_remove):
    """íŠ¹ì • ë¬¸ì„œë¥¼ ì œì™¸í•˜ê³  vectorstore ì¬êµ¬ì¶•"""
    try:
        document_list = load_document_list()
        if doc_to_remove not in document_list:
            return False
            
        print(f"ğŸ”„ Vectorstore ì¬êµ¬ì¶• ì‹œì‘ (ì œì™¸ ë¬¸ì„œ: {doc_to_remove})")
        
        # ë‚¨ì€ ë¬¸ì„œë“¤ë¡œ ìƒˆë¡œìš´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        remaining_docs = []
        for doc_name in document_list:
            if doc_name != doc_to_remove:
                file_path = Path(f"../documents/{doc_name}")
                if file_path.exists():
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¬¸ì„œ ë¡œë“œ
                    if file_path.suffix == '.pdf':
                        loader = PyPDFLoader(str(file_path))
                    elif file_path.suffix == '.docx':
                        loader = Docx2txtLoader(str(file_path))
                    elif file_path.suffix == '.pptx':
                        loader = UnstructuredPowerPointLoader(str(file_path))
                    elif file_path.suffix == '.json':
                        loader = JSONLoader(file_path=str(file_path), jq_schema='.[]', text_content=False)
                    elif file_path.suffix == '.csv':
                        loader = CSVLoader(str(file_path), encoding='utf-8')
                    elif file_path.suffix == '.xlsx':
                        df = pd.read_excel(file_path)
                        documents = []
                        for _, row in df.iterrows():
                            content = f"í…Œì´ë¸”ëª…: {row['í…Œì´ë¸”ëª…']}, ì»¬ëŸ¼ëª…: {row['ì»¬ëŸ¼ëª…']}, ì»¬ëŸ¼í•œê¸€ëª…: {row['ì»¬ëŸ¼í•œê¸€ëª…']}, ì„¤ëª…: {row['ì»¬ëŸ¼ì„¤ëª…']}, ë°ì´í„° íƒ€ì…: {row['DATATYPE']}"
                            documents.append(Document(page_content=content, metadata={"source": str(file_path)}))
                        remaining_docs.extend(documents)
                        continue
                    elif file_path.suffix == '.txt':
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        documents = [Document(page_content=content, metadata={"source": str(file_path)})]
                        remaining_docs.extend(documents)
                        continue
                    
                    documents = loader.load()
                    remaining_docs.extend(documents)
                    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {doc_name}")
        
        # ë‚¨ì€ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° vectordb íŒŒì¼ ì‚­ì œ
        if not remaining_docs:
            vectordb_path = Path(VECTOR_DB_BASE_PATH)
            if vectordb_path.exists():
                import shutil
                shutil.rmtree(vectordb_path)
                print("âœ… ë§ˆì§€ë§‰ ë¬¸ì„œ ì‚­ì œë¡œ vectordb í´ë” ì œê±°")
            return True
            
        # ë‚¨ì€ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° vectorstore ì¬êµ¬ì¶•
        text_chunks = get_text_chunks(remaining_docs)
        vectorstore = get_vectorstore(text_chunks)
        vectorstore.save_local(VECTOR_DB_BASE_PATH)
        
        print(f"âœ… Vectorstore ì¬êµ¬ì¶• ì™„ë£Œ (ì´ {len(text_chunks)} chunks)")
        return True
    
    except Exception as e:
        print(f"âŒ Vectorstore ì¬êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    
    
def load_document_list():
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë¡œë“œ"""
    try:
        with open("./document_list.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []


def save_document_list(document_list):
    """ë¬¸ì„œ ëª©ë¡ ì €ì¥"""
    with open("./document_list.json", "w", encoding="utf-8") as f:
        json.dump(list(set(document_list)), f, ensure_ascii=False, indent=2)
      
        
def get_vectorstore(text_chunks):
    """ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb


def get_vector_db_path(thread_id):
    """ì„¸ì…˜ ID ê¸°ë°˜ìœ¼ë¡œ ê°œë³„ ë²¡í„°DB ê²½ë¡œ ìƒì„±"""
    return os.path.join(VECTOR_DB_SESSION_PATH, f"{thread_id}_vectorstore")

def initialize_vector_store(thread_id):
    """ì„¸ì…˜ë³„ FAISS ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¶ˆëŸ¬ì˜¤ê¸°"""
    vector_db_path = get_vector_db_path(thread_id)
    
    if os.path.exists(vector_db_path):
        # print(f"ğŸ”¢ [initialize_vector_store] ë²¡í„°DB ë¡œë“œ ì‹œì‘ (ì„¸ì…˜: {thread_id})")
        return FAISS.load_local(vector_db_path, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
    else:
        # print(f"ğŸ”¢ [initialize_vector_store] ë²¡í„°DB ì´ˆê¸°í™” ì‹œì‘ (ì„¸ì…˜: {thread_id})")
        return FAISS.from_texts([""], OpenAIEmbeddings())

def save_chat_to_vector_db(internal_id, query, response):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ AI ì‘ë‹µì„ ë²¡í„°DBì— ì €ì¥"""
    try:
        # print(f"ğŸ”¢ [save_chat_to_vector_db] ë²¡í„°DB ì €ì¥ ì‹œì‘ (ì„¸ì…˜: {thread_id})")
        # print(f"ğŸ”¢ [save_chat_to_vector_db] ë²¡í„°DB ì €ì¥ ì‹œì‘ (query: {query})")
        # print(f"ğŸ”¢ [save_chat_to_vector_db] ë²¡í„°DB ì €ì¥ ì‹œì‘ (ì‘ë‹µ:\n {response})")
        vectorstore = initialize_vector_store(internal_id)  # ì„¸ì…˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
        
        # responseê°€ ë¬¸ìì—´ì¸ ê²½ìš°ì™€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ë¥¼ ëª¨ë‘ ì²˜ë¦¬
        if isinstance(response, str):
            document_text = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}
AI ì‘ë‹µ: {response}
"""
        else:
            document_text = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}
AI ì‘ë‹µ: {response.get("content", "ì‘ë‹µ ì—†ìŒ")}
ì‹¤í–‰ëœ ì½”ë“œ: {response.get("validated_code", "ì½”ë“œ ì—†ìŒ")}
ë¶„ì„ ê²°ê³¼: {response.get("analytic_result", "ê²°ê³¼ ì—†ìŒ")}
ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {response.get("insights", "ì¸ì‚¬ì´íŠ¸ ì—†ìŒ")}
ë¦¬í¬íŠ¸: {response.get("report", "ë¦¬í¬íŠ¸ ì—†ìŒ")}
"""
        
        # âœ… ë²¡í„°DBì— ì €ì¥
        vectorstore.add_texts([document_text])
        vector_db_path = os.path.join(VECTOR_DB_SESSION_PATH, f"{internal_id}_vectorstore")
        vectorstore.save_local(vector_db_path)

        print(f"âœ… ë²¡í„°DB ì €ì¥ ì™„ë£Œ (ì„¸ì…˜: {internal_id})")
    except Exception as e:
        print(f"âŒ [save_chat_to_vector_db] ë²¡í„°DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def summarize_retrieved_documents(filtered_results, query):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì„ í™œìš©í•˜ì—¬ ìš”ì•½"""
    if not filtered_results:
        return ""

    document_texts = "\n\n".join([
        f"[ìœ ì‚¬ë„: {score:.2f}]\n{doc.page_content}" 
        for doc, score in filtered_results
    ])

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½
    prompt = f"""
ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ì—­ì—ì„œ í˜„ì¬ ì§ˆë¬¸ "{query}"ì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ë¶€ë¶„ë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

1. í˜„ì¬ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
2. ì½”ë“œ ë¸”ë¡ê³¼ ê·¸ ì„¤ëª…ì€ ì˜¨ì „íˆ ë³´ì¡´í•˜ì„¸ìš”.
3. ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë†’ì€ ë‚´ìš©ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë‘ì„¸ìš”.
4. ì •ë³´ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”:
- í•µì‹¬ ê°œë…/ìš©ì–´ ì„¤ëª…
- ê´€ë ¨ ì½”ë“œ ì˜ˆì‹œ
- ì£¼ìš” ì¸ì‚¬ì´íŠ¸/íŒ
5. ê¸°ìˆ ì  ì •í™•ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì •ë³´ëŠ” ì œê±°í•˜ì„¸ìš”.
6. ìµœì‹  ëŒ€í™” ë‚´ìš©ì„ ë” ê´€ë ¨ì„± ë†’ê²Œ ì²˜ë¦¬í•˜ì„¸ìš”.

{document_texts}
    """
    openai_api_key = os.getenv("OPENAI_API_KEY")
    llm = ChatOpenAI(model="gpt-4o", openai_api_key=openai_api_key, temperature=0.0)
    summarized_result = llm.invoke(prompt)
    return summarized_result.content.strip()


def search_similar_questions(internal_id, query, top_k=5, similarity_threshold=0.7):
    """ë²¡í„°DBì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰"""
    vectorstore = initialize_vector_store(internal_id)  # ì„¸ì…˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    
    # ğŸ” ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ ì‹¤í–‰
    search_results = vectorstore.similarity_search_with_score(query, k=top_k*2)  # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
    
    # ìœ ì‚¬ë„ ì ìˆ˜ê°€ thresholdë¥¼ ë„˜ëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
    filtered_results = []
    seen_content = set()  # ì¤‘ë³µ ì½˜í…ì¸  í™•ì¸ìš© ì§‘í•©
    
    for doc, score in search_results:
        # FAISSì˜ scoreëŠ” L2 ê±°ë¦¬ì´ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 - score/2ê°€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì˜ ê·¼ì‚¬ê°’)
        cosine_sim = 1 - (score / 2)
        
        if cosine_sim >= similarity_threshold:
            # ì½˜í…ì¸  í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ (ì‚¬ìš©ì ì§ˆë¬¸ ë¶€ë¶„ë§Œ)
            content_key = ""
            for line in doc.page_content.split('\n'):
                if "ì‚¬ìš©ì ì§ˆë¬¸:" in line:
                    content_key = line.strip()
                    break
            
            # ì¤‘ë³µ ì½˜í…ì¸  ê±´ë„ˆë›°ê¸°
            if content_key and content_key in seen_content:
                continue
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì½˜í…ì¸  í’ˆì§ˆ ê¸°ë°˜)
            weight = 1.0
            if "validated_code: None" in doc.page_content or "ì½”ë“œ ì—†ìŒ" in doc.page_content:
                weight *= 0.8  # ì½”ë“œê°€ ì—†ëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ê°ì†Œ
            
            if "ì¸ì‚¬ì´íŠ¸: None" in doc.page_content or "ì¸ì‚¬ì´íŠ¸ ì—†ìŒ" in doc.page_content:
                weight *= 0.9  # ì¸ì‚¬ì´íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ê°ì†Œ
                
            if "ì‹¤í–‰ëœ ì½”ë“œ:" in doc.page_content and "ì½”ë“œ ì—†ìŒ" not in doc.page_content:
                weight *= 1.3  # ì‹¤í–‰ëœ ì½”ë“œê°€ ìˆëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì¦ê°€
                
            if "ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸:" in doc.page_content and "ì¸ì‚¬ì´íŠ¸ ì—†ìŒ" not in doc.page_content:
                weight *= 1.2  # ì¸ì‚¬ì´íŠ¸ê°€ ìˆëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì¦ê°€
            
            # ìµœì¢… ìŠ¤ì½”ì–´ ì¡°ì •
            adjusted_score = cosine_sim * weight
            
            if content_key:
                seen_content.add(content_key)
            
            filtered_results.append((doc, adjusted_score))
    
    # ì¡°ì •ëœ ì ìˆ˜ë¡œ ìƒìœ„ ê²°ê³¼ ì„ íƒ
    filtered_results.sort(key=lambda x: x[1], reverse=True)
    filtered_results = filtered_results[:3]  # ìƒìœ„ top_kê°œë§Œ ìœ ì§€
    
    # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    if filtered_results:
        retrieved_context = "\n\n".join([
            f"[ìœ ì‚¬ë„: {score:.2f}]\n{doc.page_content}" 
            for doc, score in filtered_results
        ])
    else:
        retrieved_context = ""
    retrieved_context = summarize_retrieved_documents(filtered_results, query)
    
    return retrieved_context


# def search_similar_questions(internal_id, query, top_k=2, similarity_threshold=0.7):
#     """ë²¡í„°DBì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰"""
#     vectorstore = initialize_vector_store(internal_id)  # ì„¸ì…˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    
#     # ğŸ” ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ ì‹¤í–‰
#     search_results = vectorstore.similarity_search_with_score(query, k=top_k)
    
#     # ìœ ì‚¬ë„ ì ìˆ˜ê°€ thresholdë¥¼ ë„˜ëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
#     filtered_results = []
#     for doc, score in search_results:
#         # FAISSì˜ scoreëŠ” L2 ê±°ë¦¬ì´ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (1 - score/2ê°€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì˜ ê·¼ì‚¬ê°’)
#         cosine_sim = 1 - (score / 2)
#         if cosine_sim >= similarity_threshold:
#             filtered_results.append((doc, cosine_sim))
    
#     # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
#     if filtered_results:
#         retrieved_context = "\n\n".join([
#             f"[ìœ ì‚¬ë„: {score:.2f}]\n{doc.page_content}" 
#             for doc, score in filtered_results
#         ])
#     else:
#         retrieved_context = ""

#     return retrieved_context
