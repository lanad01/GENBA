# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import json
from glob import glob
from pathlib import Path

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter # type: ignore
from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader, 
    UnstructuredPowerPointLoader,
    JSONLoader,
    CSVLoader
)

def load_vectorstore():
    if os.path.exists("./vectordb"):
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        try:
            return FAISS.load_local("./vectordb", embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            print(f"âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    return None


def get_text(docs):
    """ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    doc_list = []
    for doc in docs:
        file_name = f'../documents/{doc.name}'
        with open(file_name, "wb") as file:
            file.write(doc.getvalue())

        if file_name.endswith('.pdf'):
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.docx'):
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()
        elif file_name.endswith('.json'):
            loader = JSONLoader(file_path=file_name, jq_schema='.[]', text_content=False)
            documents = loader.load()
        elif file_name.endswith('.csv'):
            loader = CSVLoader(file_name, encoding='utf-8')
            documents = loader.load()
        elif file_name.endswith('.xlsx'):
            df = pd.read_excel(file_name)
            documents = []
            for _, row in df.iterrows():
                content = f"í…Œì´ë¸”ëª…: {row['í…Œì´ë¸”ëª…']}, ì»¬ëŸ¼ëª…: {row['ì»¬ëŸ¼ëª…']}, ì»¬ëŸ¼í•œê¸€ëª…: {row['ì»¬ëŸ¼í•œê¸€ëª…']}, ì„¤ëª…: {row['ì»¬ëŸ¼ì„¤ëª…']}, ë°ì´í„° íƒ€ì…: {row['DATATYPE']}"
                documents.append(Document(page_content=content, metadata={"source": file_name}))
        elif file_name.endswith('.txt'):
            with open(file_name, "r", encoding="utf-8") as f:
                content = f.read()
            documents = [Document(page_content=content, metadata={"source": file_name})]
        
        doc_list.extend(documents)
    return doc_list

def get_text_chunks(text):
    """í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100
    )
    return text_splitter.split_documents(text)

def rebuild_vectorstore_without_document(doc_to_remove):
    """íŠ¹ì • ë¬¸ì„œë¥¼ ì œì™¸í•˜ê³  vectorstore ì¬êµ¬ì¶•"""
    try:
        document_list = load_document_list()
        if doc_to_remove not in document_list:
            return False
            
        print(f"ğŸ”„ Vectorstore ì¬êµ¬ì¶• ì‹œì‘ (ì œì™¸ ë¬¸ì„œ: {doc_to_remove})")
        
        # ë‚¨ì€ ë¬¸ì„œë“¤ë¡œ ìƒˆë¡œìš´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        remaining_docs = []
        for doc_name in document_list:
            if doc_name != doc_to_remove:
                file_path = Path(f"../documents/{doc_name}")
                if file_path.exists():
                    # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¬¸ì„œ ë¡œë“œ
                    if file_path.suffix == '.pdf':
                        loader = PyPDFLoader(str(file_path))
                    elif file_path.suffix == '.docx':
                        loader = Docx2txtLoader(str(file_path))
                    elif file_path.suffix == '.pptx':
                        loader = UnstructuredPowerPointLoader(str(file_path))
                    elif file_path.suffix == '.json':
                        loader = JSONLoader(file_path=str(file_path), jq_schema='.[]', text_content=False)
                    elif file_path.suffix == '.csv':
                        loader = CSVLoader(str(file_path), encoding='utf-8')
                    elif file_path.suffix == '.xlsx':
                        df = pd.read_excel(file_path)
                        documents = []
                        for _, row in df.iterrows():
                            content = f"í…Œì´ë¸”ëª…: {row['í…Œì´ë¸”ëª…']}, ì»¬ëŸ¼ëª…: {row['ì»¬ëŸ¼ëª…']}, ì»¬ëŸ¼í•œê¸€ëª…: {row['ì»¬ëŸ¼í•œê¸€ëª…']}, ì„¤ëª…: {row['ì»¬ëŸ¼ì„¤ëª…']}, ë°ì´í„° íƒ€ì…: {row['DATATYPE']}"
                            documents.append(Document(page_content=content, metadata={"source": str(file_path)}))
                        remaining_docs.extend(documents)
                        continue
                    elif file_path.suffix == '.txt':
                        with open(file_path, "r", encoding="utf-8") as f:
                            content = f.read()
                        documents = [Document(page_content=content, metadata={"source": str(file_path)})]
                        remaining_docs.extend(documents)
                        continue
                    
                    documents = loader.load()
                    remaining_docs.extend(documents)
                    print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {doc_name}")
        
        # ë‚¨ì€ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° vectordb íŒŒì¼ ì‚­ì œ
        if not remaining_docs:
            vectordb_path = Path("./vectordb")
            if vectordb_path.exists():
                import shutil
                shutil.rmtree(vectordb_path)
                print("âœ… ë§ˆì§€ë§‰ ë¬¸ì„œ ì‚­ì œë¡œ vectordb í´ë” ì œê±°")
            return True
            
        # ë‚¨ì€ ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° vectorstore ì¬êµ¬ì¶•
        text_chunks = get_text_chunks(remaining_docs)
        vectorstore = get_vectorstore(text_chunks)
        vectorstore.save_local("./vectordb")
        
        print(f"âœ… Vectorstore ì¬êµ¬ì¶• ì™„ë£Œ (ì´ {len(text_chunks)} chunks)")
        return True
    
    except Exception as e:
        print(f"âŒ Vectorstore ì¬êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False
    
    
    
def load_document_list():
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ë¡œë“œ"""
    try:
        with open("./document_list.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []


def save_document_list(document_list):
    """ë¬¸ì„œ ëª©ë¡ ì €ì¥"""
    with open("./document_list.json", "w", encoding="utf-8") as f:
        json.dump(list(set(document_list)), f, ensure_ascii=False, indent=2)
      
        
def get_vectorstore(text_chunks):
    """ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb
