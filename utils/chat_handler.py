from datetime import datetime
import traceback
import streamlit as st
from typing import Dict, Any, Optional, Union
from langchain.memory import ConversationBufferMemory
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi

# ì‚¬ìš©ì íŒ¨í‚¤ì§€
from utils.vector_handler import save_chat_to_vector_db, search_similar_questions
from utils.thread_handler import rename_thread, save_thread
# âœ… MongoDB Atlas ì—°ê²° ì„¤ì •
uri = "mongodb+srv://swkwon:1q2w3e$r@cluster0.3rvbn.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
client = MongoClient(uri, server_api=ServerApi('1'))
db = client["chat_history"]
collection = db["conversations"]

# âœ… ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (thread_idë³„ë¡œ ê´€ë¦¬)
memory_store = {}

def get_memory(thread_id: str) -> ConversationBufferMemory:
    """
    íŠ¹ì • thread_idì— ëŒ€í•œ ConversationBufferMemoryë¥¼ ë°˜í™˜.
    ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±.
    """
    if thread_id not in memory_store:
        memory_store[thread_id] = ConversationBufferMemory(memory_key=f"history_{thread_id}", return_messages=True)
    
        # âœ… MongoDBì—ì„œ ì´ì „ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
        existing_messages = collection.find({"internal_id": thread_id}).sort("timestamp", 1)  # ì‹œê°„ìˆœ ì •ë ¬
        if existing_messages:
            for document in existing_messages:
                for msg in document.get("messages", []):
                    if msg["role"] == "user":
                        memory_store[thread_id].chat_memory.add_user_message(msg["content"])
                    elif msg["role"] == "assistant":
                        memory_store[thread_id].chat_memory.add_ai_message(msg["content"])

    return memory_store[thread_id]

# âœ… ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬
def handle_chat_response(
    assistant: Any,
    query: str,
    internal_id: str
) -> tuple[Optional[Dict[str, Any]], ConversationBufferMemory]:
    """
    ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬ (ê¸°ì¡´ ì§ˆë¬¸ê³¼ AI ì‘ë‹µì„ ê¸°ì–µ)
    
    Args:
        assistant: ì±—ë´‡ ì–´ì‹œìŠ¤í„´íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        query: ì‚¬ìš©ì ì§ˆë¬¸
        thread_id: ìŠ¤ë ˆë“œ ID

    Returns:
        tuple[ì‘ë‹µ ë°ì´í„° ë”•ì…”ë„ˆë¦¬, ì—…ë°ì´íŠ¸ëœ ë©”ëª¨ë¦¬ ê°ì²´]
    """
    try:
        print(f"ğŸ•› [handle_chat_response] ì§ˆë¬¸ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # âœ… thread_idë³„ memory ê°€ì ¸ì˜¤ê¸°
        memory = get_memory(internal_id)

        # âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        messages = memory.load_memory_variables({}).get(f"history_{internal_id}", "")

        # ë©”ì‹œì§€ ê°ì²´ë¥¼ ì½ê¸° ì‰¬ìš´ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        previous_context = ""
        if messages:
            for msg in messages:
                if msg.type == 'human':
                    previous_context += f"ì‚¬ìš©ì: {msg.content}\n"
                elif msg.type == 'ai':
                    previous_context += f"ì–´ì‹œìŠ¤í„´íŠ¸: {msg.content}\n"

        ##########################################################################################
        # âœ… Context Window ì²˜ë¦¬
        # ** í•´ë‹¹ ì“°ë ˆë“œì˜ ì§ˆë¬¸-ë‹µë³€ ì´ë ¥ì´ ìŒ“ì—¬ìˆëŠ” ë²¡í„°DBì—ì„œ ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ ê²€ìƒ‰
        ##########################################################################################
        model = assistant.llm
        filtered_results = search_similar_questions(internal_id, query)
        if not filtered_results:
            return ""

        document_texts = "\n\n".join([
            f"[ìœ ì‚¬ë„: {score:.2f}]\n{doc.page_content}" 
            for doc, score in filtered_results
        ])

        # LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ìš”ì•½
        prompt = f"""
ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ì—­ì—ì„œ í˜„ì¬ ì§ˆë¬¸ "{query}"ì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
ì´ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ê³¼ ì§ì ‘ ì—°ê²°ë˜ëŠ” í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.

1. ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ” ì •ë³´ë§Œ ë‚¨ê¸°ê³  ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œê±°
2. ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ì½”ë“œê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
3. ë¶„ì„ ê²°ê³¼ë‚˜ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ëŠ” ì •ë¦¬í•´ì„œ í¬í•¨
4. ì •ë³´ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”:
    - í•µì‹¬ ê°œë… ì„¤ëª…
    - ê´€ë ¨ ì½”ë“œ
    - ì£¼ìš” ì¸ì‚¬ì´íŠ¸(3ì¤„ ì´ë‚´)

{document_texts}
        """
        summarized_result = model.invoke(prompt).content.strip()
        
        ##########################################################################################
        # âœ… Query Rewriting
        #  """ ê¸°ì¡´ ë¬¸ë§¥ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„± """
        ##########################################################################################
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ê³  ê²€ìƒ‰ëœ ë¬¸ì„œë„ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if not previous_context or not summarized_result:
            final_query = query  
        # ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ìˆê³  ê²€ìƒ‰ëœ ë¬¸ì„œë„ ìˆìœ¼ë©´ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±
        else:
            prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.
ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë³´ë‹¤ ëª…í™•í•˜ê³  ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.

[ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½]
{summarized_result}

[ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸]
{query}

[ì¬êµ¬ì„±ëœ ì§ˆë¬¸]
            """
            final_query = model.invoke(prompt).content.strip()

        print(f"ğŸ¤µ ì¬êµ¬ì„±ëœ ì§ˆë¬¸:\n{final_query}")
        result = assistant.ask(final_query)
        print(f"ğŸ¤µ ê²°ê³¼:\n{result}")
        
        # âœ… UI ë Œë”ë§ì„ ìœ„í•´ ë‹µë³€ ê²°ê³¼(result)ë¥¼ messages ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì €ì¥
        response_data = {
            "role": "assistant",
            "content": "ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",  # ê¸°ë³¸ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
            "validated_code": result.get("validated_code"),
            "analytic_result": result.get("analytic_result"),
            "chart_filename": result.get("chart_filename"),
            "insights": result.get("insights"),
            "report": result.get("report"),
            "request_summary": result.get("request_summary"),
        }

        # âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µì¼ ê²½ìš° ì²˜ë¦¬
        if "analytic_result" not in result:
            if "error_message" in result:
                response_data["content"] = result["error_message"]
                st.error(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {result['error_message']}")
            else:
                response_data["content"] = (
                    result.get("general_response") or 
                    result.get("knowledge_response") or 
                    result.get("response", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                )

        # âœ… ë©”ëª¨ë¦¬ì— ì§ˆë¬¸ ë° ì‘ë‹µ ì €ì¥ (ì—ëŸ¬ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if "error_message" not in response_data:
            if isinstance(query, str):
                memory.chat_memory.add_user_message(query)
            if isinstance(response_data["content"], str):
                memory.chat_memory.add_ai_message(response_data["content"])

            # âœ… MongoDBì—ë„ ëŒ€í™” ì´ë ¥ ì €ì¥
            collection.update_one(
                {"internal_id": internal_id},
                {
                    "$push": {
                        "messages": {
                            "$each": [
                                {"role": "user", "content": query, "timestamp": datetime.now()},
                                {"role": "assistant", "content": response_data["content"], "timestamp": datetime.now()}
                            ]
                        }
                    }
                },
                upsert=True
            )
            
            # âœ… ì‘ë‹µì—ì„œ request_summary í™•ì¸ ë° thread_id ë³€ê²½
            if "request_summary" in response_data:
                rename_thread(internal_id, response_data["request_summary"])

            # âœ… ë²¡í„°DB ì €ì¥
            save_chat_to_vector_db(internal_id, query, response_data)

        return response_data

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ [handle_chat_response] : {traceback.format_exc()}")
        return None, None
